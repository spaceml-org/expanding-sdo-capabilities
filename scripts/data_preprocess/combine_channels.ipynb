{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script that will merge all the channel NPZ files together for a single time step on Google's GCP storage buckets to efficiently fetch a single time step in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Move this into an actual script rather than a notebook.\n",
    "\n",
    "#!env python\n",
    "\"\"\"\n",
    "Script that will merge all the channel NPZ files together for a single time step on Google's GCP storage buckets\n",
    "to efficiently fetch a single time step in one go.\n",
    "\n",
    "Before running this, run the following command on your remote GCP compute instance:\n",
    "gcloud auth application-default login\n",
    "\n",
    "Note that you must also have the correct GCP access scopes set for your service provider\n",
    "account for this script to work:\n",
    "* Compute Engine - Read Write\n",
    "* Storage - Full\n",
    "\n",
    "Make sure the project wide service account also has the right cloud storage roles. Run\n",
    "this on your local laptop:\n",
    "export SERVICE_ACCOUNT=524456442905-compute@developer.gserviceaccount.com\n",
    "export BUCKET_NAME=fdl-sdo-data\n",
    "gsutil iam ch serviceAccount:$SERVICE_ACCOUNT:roles/storage.objectAdmin gs://$BUCKET_NAME\n",
    "gsutil iam ch serviceAccount:$SERVICE_ACCOUNT:roles/storage.objectCreator gs://$BUCKET_NAME\n",
    "gsutil iam ch serviceAccount:$SERVICE_ACCOUNT:roles/storage.objectViewer gs://$BUCKET_NAME\n",
    "\n",
    "When you run this script, ensure you send its output to a log:\n",
    "python ./combine_channels.py > /tmp/combine_output.log 2>&1 & tail -f /tmp/combine_output.log\n",
    "Grep the log to make sure no \"FINAL ERROR!!!!!!!!\" messages are in there; if they are, they indicate\n",
    "holes in your processed dataset due to un-fixable retries.\n",
    "\n",
    "\n",
    "Example of how you'd reload one of these compressed all-in-one channel files after this script\n",
    "is finished pre-processing everything:\n",
    "\n",
    "client = storage.Client()\n",
    "bucket = client.get_bucket('fdl-sdo-data')\n",
    "filename = 'SDOMLnpz/2010/05/01/HMI20100501_0036_all.pklz'\n",
    "blob = bucket.get_blob(filename)\n",
    "results = pickle.loads(bz2.decompress(blob.download_as_string()))\n",
    "\n",
    "'results' is an array of tuples, where each tuple contains the string of the channel name,\n",
    "such as 'bx', with the numpy array of its image results at 512x512 resolution.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import bz2\n",
    "import concurrent\n",
    "import itertools\n",
    "from io import BytesIO\n",
    "import multiprocessing as mp\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Pointer to where 'gcloud auth login' put credentials.\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = os.path.expanduser(\n",
    "    '~/expanding-sdo-capabilities/config/space_weather_sdo.json')\n",
    "\n",
    "# The number of processes we want to spawn in parallel to handle converting timestamp chunks.\n",
    "NUM_CONSUMERS = mp.cpu_count() - 1\n",
    "\n",
    "# The number of timestamp channel groups we want to download and convert in one shot.\n",
    "NUM_TIMESTAMPS_AT_ONCE_PER_PROCESS = 10\n",
    "\n",
    "# Bucket name of where we store our SDO data.\n",
    "GCP_BUCKET_NAME = 'fdl-sdo-data'\n",
    "\n",
    "# Path to NPZ inventory for quick look ups.\n",
    "GCP_INVENTORY_IN_PATH = 'SDOMLnpz/inventory.pkl'\n",
    "\n",
    "# Set to a number like 20 to test processing a subset of data during testing.\n",
    "# TODO!!! Set back to None when we are done debugging.\n",
    "MAX_ITER = 20\n",
    "#MAX_ITER = None\n",
    "\n",
    "# Lock to safely have threads and processes print to the screen.\n",
    "global console_lock\n",
    "console_lock = mp.Lock()\n",
    "\n",
    "\n",
    "def safeprint(msg):\n",
    "    \"\"\"\n",
    "    Make sure multiple processes don't step on each others feet when printing out to the\n",
    "    console.\n",
    "    \"\"\"\n",
    "    with console_lock:\n",
    "        print(msg)\n",
    "\n",
    "        \n",
    "def enumerate_timestamp_channels(df, max_iter):\n",
    "    \"\"\"\n",
    "    Group each of the timestamps into all of their channels, returning\n",
    "    the results chunked together to handle as a group of channels per\n",
    "    timestamp. max_iter is useful for debugging to limit the number of\n",
    "    results we iterate and produce.\n",
    "    \"\"\"\n",
    "    indexes = ['year', 'month', 'day', 'hour', 'min']\n",
    "    counter = 0\n",
    "    for _, timestamp_group in df.groupby(indexes):\n",
    "        channels = [\n",
    "            {'channel': row['channel'], 'file': row['file']}\n",
    "            for _, row in timestamp_group.iterrows()\n",
    "        ]\n",
    "        yield channels\n",
    "        \n",
    "        counter += 1\n",
    "        if max_iter is not None and counter >= max_iter:\n",
    "            break\n",
    "\n",
    "            \n",
    "def chunked_iterable(iterable, chunk_size):\n",
    "    \"\"\"\n",
    "    Iterate through something in 'chunks', where each chunk is returned as a group.\n",
    "    \"\"\"\n",
    "    it = iter(iterable)\n",
    "    while True:\n",
    "        chunk = tuple(itertools.islice(it, chunk_size))\n",
    "        if not chunk:\n",
    "            break\n",
    "        yield chunk\n",
    "\n",
    "        \n",
    "def connect_gcp(gcp_bucket_name):\n",
    "    \"\"\"\n",
    "    Connect to the Google Cloud Provider storage bucket. Note\n",
    "    that the environment variable GOOGLE_APPLICATION_CREDENTIALS must\n",
    "    be set to the path to a GCP JSON config file before this is run,\n",
    "    such as:\n",
    "    export GOOGLE_APPLICATION_CREDENTIALS=~/expanding-sdo-capabilities/config/space_weather_sdo.json\n",
    "    \"\"\"\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(gcp_bucket_name)\n",
    "    return client, bucket\n",
    "\n",
    "\n",
    "def get_combined_path(channel_path):\n",
    "    \"\"\"\n",
    "    Given a path to a channel, generates an *_all.pklz version of that path pointing\n",
    "    to the combined channels file.\n",
    "    \"\"\"\n",
    "    return re.sub(r'_[^._]*?\\.npz', '_all.pklz', channel_path)\n",
    "\n",
    "\n",
    "# Maintain a single producer process and multiple consumer processes.\n",
    "# Use a single blocking queue to coordinate the work from the producer to the consumer\n",
    "# processes. The producer will keep putting chunks of timesteps to process, blocking\n",
    "# when the queue gets too full, and the consumers will read from this queue.\n",
    "#\n",
    "# Each consumer process will also spawn threads inside of itself to deal with its\n",
    "# chunked items in parallel. This means we get to use multiple cores on the system via\n",
    "# Python processes for the consumer processes, as well as being efficient in terms of\n",
    "# being network/IO-bound using Python threads.\n",
    "\n",
    "\n",
    "def producer(df, work_queue, max_iter):\n",
    "    # For each timestep group all of its channels together, then group these into chunks that\n",
    "    # we can efficiently process all at once. Put these into the work queue for consumers\n",
    "    # to process.\n",
    "    safeprint('Producer setting up iterable chunks...')\n",
    "    channels = chunked_iterable(enumerate_timestamp_channels(df, max_iter=max_iter),\n",
    "                                chunk_size=NUM_TIMESTAMPS_AT_ONCE_PER_PROCESS)\n",
    "    safeprint('Producer finished setting up iterable chunks')\n",
    "    for chunk in channels:\n",
    "        work_queue.put(chunk)\n",
    "    \n",
    "    # TODO: Get the updated final list of combined paths and update the inventory\n",
    "    # path, then re-upload that.\n",
    "    \n",
    "    for idx in range(NUM_CONSUMERS):\n",
    "        work_queue.put('DONE')\n",
    "\n",
    "        \n",
    "def consumer(process_idx, work_queue):\n",
    "    \"\"\"\n",
    "    Note: this method runs in its own process.\n",
    "    \"\"\"\n",
    "    safeprint('Consumer {} started'.format(process_idx))\n",
    "    # Note: GCP connections are thread-safe but not multi-process safe.\n",
    "    client, bucket = connect_gcp(GCP_BUCKET_NAME)\n",
    "    while True:\n",
    "        msg = work_queue.get()\n",
    "        if msg == 'DONE':\n",
    "            safeprint('Consumer {} DONE'.format(process_idx))\n",
    "            break\n",
    "        \n",
    "        chunks = msg\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=len(chunks)) as executor:\n",
    "            executor.map(lambda chunk: process_timestamp(process_idx, chunk, bucket), chunks)\n",
    "        safeprint('Process {} finished with its threads'.format(process_idx))\n",
    "\n",
    "        \n",
    "def process_timestamp(process_idx, channels, bucket, max_retries=5, wait_time_s=5):\n",
    "    \"\"\"\n",
    "    For each timestamp, download all of its channels in parallel then combine them into a single\n",
    "    file we can re-upload back to GCP.\n",
    "\n",
    "    Note: this runs in its own thread, managed by the consumer() method.\n",
    "    \"\"\"\n",
    "    \n",
    "    def download_channel(process_idx, channel_name, channel_path, bucket, max_retries=5, wait_time_s=5):\n",
    "        \"\"\"\n",
    "        Download an individual timestamps channel.\n",
    "        \n",
    "        Note: this runs in its own thread, managed by the process_timestamp method().\n",
    "        \"\"\"\n",
    "        for retry in range(max_retries):\n",
    "            try:\n",
    "                img_data = bucket.blob(channel_path).download_as_string()\n",
    "                img = np.load(BytesIO(img_data))['x']\n",
    "                return (channel_name, img)\n",
    "            except Exception as e:\n",
    "                safeprint('Consumer {}: ERROR in download_channel thread for {}: {}, waiting {} seconds for retry {}'\n",
    "                          .format(process_idx, channel_path, e, wait_time_s, retry))\n",
    "                time.sleep(wait_time_s)\n",
    "        \n",
    "        safeprint('ERROR!!!!!!!! Even after retries unable to fully run download_channel for {}'\n",
    "                  .format(channel_path))\n",
    "        \n",
    "    for retry in range(max_retries):\n",
    "        try:\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=len(channels)) as executor:\n",
    "                runme = lambda channel: download_channel(process_idx, channel['channel'], channel['file'], bucket)\n",
    "                results = executor.map(runme, channels)\n",
    "\n",
    "            # Ensure we always sort the list of channels consistently by the channel name.\n",
    "            results = list(results)\n",
    "            results = sorted(results, key=lambda entry: entry[0])\n",
    "            \n",
    "            # Pickle, compress, and upload these to GCP as a single combined channel file.\n",
    "            compressed_out = bz2.compress(pickle.dumps(results))\n",
    "            combined_path = get_combined_path(channels[0]['file'])\n",
    "            blob = bucket.blob(combined_path)\n",
    "            blob.upload_from_string(compressed_out)        \n",
    "\n",
    "            safeprint('Consumer {}: Finished {}'.format(process_idx, combined_path))\n",
    "            return\n",
    "        except Exception as e:\n",
    "            safeprint('Consumer {}: ERROR in process_timestamp thread for {}: {}, waiting {} seconds for retry {}'\n",
    "                      .format(process_idx, channels, e, wait_time_s, retry))\n",
    "            time.sleep(wait_time_s)\n",
    "\n",
    "        safeprint('FINAL ERROR!!!!!!!! Even after retries unable to fully run process_timestamp for {}'\n",
    "                  .format(channels))\n",
    "\n",
    "\n",
    "def update_inventory(df, bucket, max_iter):\n",
    "    \"\"\"\n",
    "    Make sure we record the *_all versions of the combined channels into the inventory for efficient lookups.\n",
    "    \"\"\"\n",
    "    safeprint('Updating paths in inventory file...')\n",
    "    for channels in enumerate_timestamp_channels(df, max_iter=max_iter):\n",
    "        pass\n",
    "        \n",
    "    safeprint('Finished updating paths in inventory file')\n",
    "    \n",
    "def main():\n",
    "    client, bucket = connect_gcp(GCP_BUCKET_NAME)\n",
    "\n",
    "    data = bucket.blob(GCP_INVENTORY_IN_PATH).download_as_string()\n",
    "    df = pd.read_pickle(BytesIO(data), compression='gzip')\n",
    "\n",
    "    work_queue = mp.Queue(maxsize=NUM_CONSUMERS)\n",
    "    consumers = [None] * NUM_CONSUMERS\n",
    "    for idx in range(NUM_CONSUMERS):\n",
    "        c = mp.Process(target=consumer, args=(idx, work_queue))\n",
    "        c.daemon = True\n",
    "        c.start()\n",
    "\n",
    "    producer(df, work_queue, MAX_ITER)\n",
    "\n",
    "    update_inventory(df, bucket, MAX_ITER)\n",
    "\n",
    "    safeprint('Main process DONE')\n",
    "\n",
    "\n",
    "\n",
    "#main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "client, bucket = connect_gcp(GCP_BUCKET_NAME)\n",
    "\n",
    "data = bucket.blob(GCP_INVENTORY_IN_PATH).download_as_string()\n",
    "df = pd.read_pickle(BytesIO(data), compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add a short-circuit where we don't re-add the 'all' channel if its already present\n",
    "# in the inventory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 entry: year                                              2010\n",
      "month                                                5\n",
      "day                                                  1\n",
      "hour                                                 0\n",
      "min                                                 12\n",
      "channel                                             by\n",
      "file       SDOMLnpz/2010/05/01/HMI20100501_0012_by.npz\n",
      "Name: 1076941452, dtype: object\n",
      "0 entry: year                                              2010\n",
      "month                                                5\n",
      "day                                                  1\n",
      "hour                                                 0\n",
      "min                                                 24\n",
      "channel                                             by\n",
      "file       SDOMLnpz/2010/05/01/HMI20100501_0024_by.npz\n",
      "Name: 1076941464, dtype: object\n",
      "0 entry: year                                              2010\n",
      "month                                                5\n",
      "day                                                  1\n",
      "hour                                                 0\n",
      "min                                                 36\n",
      "channel                                             bx\n",
      "file       SDOMLnpz/2010/05/01/HMI20100501_0036_bx.npz\n",
      "Name: 1076941476, dtype: object\n",
      "0 entry: year                                              2010\n",
      "month                                                5\n",
      "day                                                  1\n",
      "hour                                                 0\n",
      "min                                                 48\n",
      "channel                                             by\n",
      "file       SDOMLnpz/2010/05/01/HMI20100501_0048_by.npz\n",
      "Name: 1076941488, dtype: object\n",
      "0 entry: year                                              2010\n",
      "month                                                5\n",
      "day                                                  1\n",
      "hour                                                 1\n",
      "min                                                  0\n",
      "channel                                             by\n",
      "file       SDOMLnpz/2010/05/01/HMI20100501_0100_by.npz\n",
      "Name: 1076941500, dtype: object\n"
     ]
    }
   ],
   "source": [
    "max_iter = 5\n",
    "indexes = ['year', 'month', 'day', 'hour', 'min']\n",
    "counter = 0\n",
    "updated_df = df.copy()\n",
    "for _, timestamp_group in df.groupby(indexes):\n",
    "    entries = list(timestamp_group.iterrows())\n",
    "    new_entry = entries[0][1].copy()\n",
    "    print('0 entry: {}'.format(new_entry))\n",
    "    new_entry['channel'] = 'all'\n",
    "    new_entry['file'] = get_combined_path(entries[0][1]['file'])\n",
    "    updated_df = updated_df.append(new_entry)\n",
    "    for (_, c) in entries:\n",
    "        updated_df = updated_df.append(c)\n",
    "    \n",
    "    counter += 1\n",
    "    if max_iter is not None and counter >= max_iter:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>min</th>\n",
       "      <th>channel</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1076941452</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>by</td>\n",
       "      <td>SDOMLnpz/2010/05/01/HMI20100501_0012_by.npz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1076941452</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>bx</td>\n",
       "      <td>SDOMLnpz/2010/05/01/HMI20100501_0012_bx.npz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1076941452</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>bz</td>\n",
       "      <td>SDOMLnpz/2010/05/01/HMI20100501_0012_bz.npz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1076941464</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>by</td>\n",
       "      <td>SDOMLnpz/2010/05/01/HMI20100501_0024_by.npz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1076941464</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>bx</td>\n",
       "      <td>SDOMLnpz/2010/05/01/HMI20100501_0024_bx.npz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1076941488</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>bx</td>\n",
       "      <td>SDOMLnpz/2010/05/01/HMI20100501_0048_bx.npz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1076941500</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>all</td>\n",
       "      <td>SDOMLnpz/2010/05/01/HMI20100501_0100_all.pklz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1076941500</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>by</td>\n",
       "      <td>SDOMLnpz/2010/05/01/HMI20100501_0100_by.npz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1076941500</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>bz</td>\n",
       "      <td>SDOMLnpz/2010/05/01/HMI20100501_0100_bz.npz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1076941500</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>bx</td>\n",
       "      <td>SDOMLnpz/2010/05/01/HMI20100501_0100_bx.npz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7345160 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              year  month  day  hour  min channel  \\\n",
       "index                                               \n",
       "1076941452  2010.0      5    1     0   12      by   \n",
       "1076941452  2010.0      5    1     0   12      bx   \n",
       "1076941452  2010.0      5    1     0   12      bz   \n",
       "1076941464  2010.0      5    1     0   24      by   \n",
       "1076941464  2010.0      5    1     0   24      bx   \n",
       "...            ...    ...  ...   ...  ...     ...   \n",
       "1076941488  2010.0      5    1     0   48      bx   \n",
       "1076941500  2010.0      5    1     1    0     all   \n",
       "1076941500  2010.0      5    1     1    0      by   \n",
       "1076941500  2010.0      5    1     1    0      bz   \n",
       "1076941500  2010.0      5    1     1    0      bx   \n",
       "\n",
       "                                                     file  \n",
       "index                                                      \n",
       "1076941452    SDOMLnpz/2010/05/01/HMI20100501_0012_by.npz  \n",
       "1076941452    SDOMLnpz/2010/05/01/HMI20100501_0012_bx.npz  \n",
       "1076941452    SDOMLnpz/2010/05/01/HMI20100501_0012_bz.npz  \n",
       "1076941464    SDOMLnpz/2010/05/01/HMI20100501_0024_by.npz  \n",
       "1076941464    SDOMLnpz/2010/05/01/HMI20100501_0024_bx.npz  \n",
       "...                                                   ...  \n",
       "1076941488    SDOMLnpz/2010/05/01/HMI20100501_0048_bx.npz  \n",
       "1076941500  SDOMLnpz/2010/05/01/HMI20100501_0100_all.pklz  \n",
       "1076941500    SDOMLnpz/2010/05/01/HMI20100501_0100_by.npz  \n",
       "1076941500    SDOMLnpz/2010/05/01/HMI20100501_0100_bz.npz  \n",
       "1076941500    SDOMLnpz/2010/05/01/HMI20100501_0100_bx.npz  \n",
       "\n",
       "[7345160 rows x 7 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remember to save the resulting inventory pkl with compression on."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
