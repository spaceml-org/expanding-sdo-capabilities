{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Standard Candle\" Toy 4: Multi-Channel MNIST\n",
    "\n",
    "Have multiple MNIST channels being fed in, where each input channel is a separate MNIST drawn digit but from the same class. The output of the network is a single MNIST class prediction across all the MNIST image channels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this notebook should be run from the `notebooks/` subdirectory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Change this to what the notebook name is for each experiment to ensure\n",
    "# training results are saved into the right sub-directory.\n",
    "notebook_name = '04b_simple_toy1'\n",
    "\n",
    "from collections import namedtuple, defaultdict\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Protective code to ensure we always reset the random seed when doing training, or else\n",
    "# code won't be reproducible.\n",
    "if 'training_runs_count' not in globals():\n",
    "  training_runs_count = 0\n",
    "if 'seed_reset_count' not in globals():\n",
    "  seed_reset_count = 0\n",
    "\n",
    "# Path to data and training results.\n",
    "root_path = '../..' # Relative to: notebooks/01b_simple_toy\n",
    "data_path = os.path.join(root_path, 'data', notebook_name)\n",
    "results_path = os.path.join(root_path, 'training_results', notebook_name)\n",
    "model_path = os.path.join(results_path, 'model.pth')\n",
    "optimizer_path = os.path.join(results_path, 'optimizer.pth')\n",
    "\n",
    "for path in [data_path, results_path]:\n",
    "  if not os.path.exists(path):\n",
    "    print('{} does not exist; creating directory...'.format(os.path.abspath(path)))\n",
    "    os.makedirs(path)\n",
    "\n",
    "n_epochs = 8\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "log_interval = 10\n",
    "\n",
    "# Number of MNIST images given to the network at once.\n",
    "# TODO: Try a larger number of channels.\n",
    "num_channels = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0 for training, current device: 0, total devices: 6\n"
     ]
    }
   ],
   "source": [
    "# Use the GPU.\n",
    "cuda_device = 0\n",
    "torch.backends.cudnn.enabled = True\n",
    "if not torch.cuda.is_available():\n",
    "  raise RuntimeError(\"CUDA not available! Unable to continue\")\n",
    "# Force ourselves to use only one GPU.\n",
    "device = torch.device(\"cuda:{}\".format(cuda_device))\n",
    "print(\"Using device {} for training, current device: {}, total devices: {}\".format(\n",
    "  device, torch.cuda.current_device(), torch.cuda.device_count()))\n",
    "\n",
    "# Force runs to be deterministic and reproducible.\n",
    "random_seed = 1\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ['PYTHONHASHSEED'] = str(random_seed)\n",
    "seed_reset_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Custom pytorch dataset that returns multi-channel MNIST results all of the same class (0 through 9) but\n",
    "with different hand-drawn images as each channel. Also applies a random brightness factor to each image.\n",
    "\"\"\"\n",
    "class MultiChannelBrightnessMNIST(torchvision.datasets.MNIST):\n",
    "  \n",
    "  SampleTuple = namedtuple(\"Sample\", \"image orig_image target brightness\")\n",
    "  \n",
    "  # TODO: Use dynamic arguments to super() to not have to hard code these params\n",
    "  # from torchvision.datsets.MNIST.\n",
    "  def __init__(self, root, num_channels=num_channels,\n",
    "               train=True, transform=None, target_transform=None, download=False):\n",
    "    super(MultiChannelBrightnessMNIST, self).__init__(\n",
    "      root, train, transform, target_transform, download)\n",
    "\n",
    "    self._num_channels = num_channels\n",
    "    self._setup_samples()\n",
    "    self._compute_statistics()\n",
    "\n",
    "  def _setup_samples(self):\n",
    "    \"\"\"\n",
    "    Re-organizes the data into tuples of the following form to make them easier to work with,\n",
    "    as well as to record what the random brightness is for correctly generating statistics\n",
    "    and ground truth information:\n",
    "\n",
    "    (image, orig_image, target, brightness)\n",
    "\n",
    "    Also creates a hashtable that goes from the target class to all available image tuples\n",
    "    with those classes so we can quickly fetch images of the same class together.\n",
    "    \"\"\"\n",
    "    self._available_samples = defaultdict(lambda: [])\n",
    "    self._all_samples = []\n",
    "    for (image, target) in zip(self.data, self.targets):\n",
    "      brightness = self._compute_brightness()\n",
    "\n",
    "      # Our loss function currently assumes that all brightness values are in the\n",
    "      # range [0.0, 1.0]. If this assumption is broken make sure we know so that\n",
    "      # in the future we can do extra work to 'squash' brightness values back\n",
    "      # down to [0.0, 1.0]\n",
    "      assert brightness >= 0.0 and brightness <= 1.0,\n",
    "        \"Brightness must be in the range [0.0, 1.0] for the loss function to work\"\n",
    "      \n",
    "      darker_image = self._darken_image(brightness, image)\n",
    "\n",
    "      sample = self.SampleTuple(image=darker_image, orig_image=image, target=target,\n",
    "                                brightness=brightness)\n",
    "\n",
    "      assert isinstance(target, torch.Tensor), \"Expected Torch Tensor type!\"\n",
    "      self._available_samples[target.item()].append(sample)\n",
    "      self._all_samples.append(sample)\n",
    "      \n",
    "  def _compute_brightness(self):\n",
    "    # Randomly compute what the brightness setting should be for each sample.\n",
    "    # NOTE: Below 30% brightness accuracy degrades vs. the experiment 2 baseline.\n",
    "    return np.round((0.3 - 1.0) * np.random.random_sample() + 1.0, decimals=2)\n",
    "\n",
    "  def _darken_image(self, brightness, img):\n",
    "    brightness = torch.tensor(brightness)\n",
    "    return img.double().mul(brightness).byte()\n",
    "  \n",
    "  def _compute_statistics(self):\n",
    "    \"\"\"\n",
    "    Compute global mean and standard deviation for data input normalization.\n",
    "    \"\"\"\n",
    "    max_pixel_value = 255.0\n",
    "    means = np.zeros((len(self._all_samples)), dtype=np.float)\n",
    "    stds = np.zeros((len(self._all_samples)), dtype=np.float)\n",
    "    for idx, sample in enumerate(self._all_samples):\n",
    "      np_img = np.array(sample.image, dtype=np.float)\n",
    "      means[idx] = np.round((np_img / max_pixel_value).mean(), decimals=4)\n",
    "      stds[idx] = np.round((np_img / max_pixel_value).std(), decimals=4)\n",
    "\n",
    "    self.global_mean = np.round(np.sum(means) / len(means), decimals=4)\n",
    "    self.global_std = np.round(np.sum(stds) / len(means), decimals=4)\n",
    "    \n",
    "    # Ensure our computed mean and std are within what we've seen historically;\n",
    "    # otherwise flag this as a potential bug.\n",
    "    assert self.global_mean >= 0.05 and self.global_mean <= 0.2,\n",
    "      \"Computed global mean is outside historical ranges seen: {}\".format(\n",
    "        self.global_mean)\n",
    "    assert self.global_std >= 0.1 and self.global_std <= 0.4,\n",
    "      \"Computed global std is outside historical ranges seen: {}\".format(\n",
    "        self.global_std)\n",
    "    \n",
    "  def __getitem__(self, index):\n",
    "    # NOTE: We return items in bunches of num_channels. We also essentially\n",
    "    # ignore the index and just return random bunches of the same class.\n",
    "\n",
    "    # Randomly choose a class label.\n",
    "    target_class = np.random.randint(low=0, high=10)\n",
    "    samples_with_class = self._available_samples[target_class]\n",
    "    \n",
    "    # Fetch num_channel samples that have this target class.\n",
    "    result_idxs = np.random.choice(len(samples_with_class), size=(self._num_channels,),\n",
    "                                   replace=False)\n",
    "    results = [samples_with_class[idx] for idx in result_idxs]\n",
    "    \n",
    "    # NOTE: If we don't cast back into a PIL Image we lose some precision vs. experiment 2; this is\n",
    "    # probably due to some image processing or slight byte conversion happening inside of PIL image.\n",
    "    # TODO: See if this is still needed in experiment 4+.\n",
    "    results = [self.SampleTuple(image=Image.fromarray(sample.image.numpy(), mode='L'),\n",
    "                                orig_image=sample.orig_image,\n",
    "                                target=sample.target,\n",
    "                                brightness=sample.brightness) for sample in results]\n",
    "    \n",
    "    if self.transform is not None:\n",
    "      results = [self.SampleTuple(image=self.transform(sample.image),\n",
    "                                  orig_image=sample.orig_image,\n",
    "                                  target=sample.target,\n",
    "                                  brightness=sample.brightness) for sample in results]\n",
    "      \n",
    "\n",
    "    if self.target_transform is not None:\n",
    "      results = [self.SampleTuple(image=self.target_transform(sample.image),\n",
    "                                  orig_image=sample.orig_image,\n",
    "                                  target=sample.target,\n",
    "                                  brightness=sample.brightness) for sample in results]\n",
    "    \n",
    "    # Turn the images into a single multi-channel image.\n",
    "    height = 28\n",
    "    width = 28\n",
    "    multi_channel_img = torch.tensor(np.zeros((self._num_channels, height, width)),\n",
    "                                     dtype=torch.float32)\n",
    "    for idx, sample in enumerate(results):\n",
    "      multi_channel_img[idx] = torch.tensor(np.array(sample.image), dtype=torch.float32)\n",
    "\n",
    "    # Make sure we don't accidentally mix classes in our results.\n",
    "    assert len(list(filter(lambda sample: sample.target != target_class, results))) == 0, \\\n",
    "           \"Classes are getting mixed up in dataset results!\"\n",
    "    \n",
    "    debug_results = results\n",
    "\n",
    "    return multi_channel_img, target_class, debug_results\n",
    "  \n",
    "  def __len__(self):\n",
    "    return math.floor(len(self.data) / self._num_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly choose a class label.\n",
    "target_class = np.random.randint(low=0, high=10)\n",
    "samples_with_class = training_data._available_samples[target_class]\n",
    "\n",
    "# Fetch num_channel samples that have this target class.\n",
    "result_idxs = np.random.choice(len(samples_with_class), size=(training_data._num_channels,), replace=False)\n",
    "results = [samples_with_class[idx] for idx in result_idxs]\n",
    "\n",
    "# NOTE: If we don't cast back into a PIL Image we lose some precision vs. experiment 2; this is\n",
    "# probably due to some image processing or slight byte conversion happening inside of PIL image.\n",
    "# TODO: See if this is still needed in experiment 4+.\n",
    "results = [training_data.SampleTuple(\n",
    "                            image=Image.fromarray(sample.image.numpy(), mode='L'),\n",
    "                            orig_image=sample.orig_image,\n",
    "                            target=sample.target,\n",
    "                            brightness=sample.brightness) for sample in results]\n",
    "\n",
    "if training_data.transform is not None:\n",
    "  results = [training_data.SampleTuple(\n",
    "                              image=training_data.transform(sample.image),\n",
    "                              orig_image=sample.orig_image,\n",
    "                              target=sample.target,\n",
    "                              brightness=sample.brightness) for sample in results]\n",
    "\n",
    "\n",
    "if training_data.target_transform is not None:\n",
    "  results = [training_data.SampleTuple(\n",
    "                              image=training_data.target_transform(sample.image),\n",
    "                              orig_image=sample.orig_image,\n",
    "                              target=sample.target,\n",
    "                              brightness=sample.brightness) for sample in results]\n",
    "\n",
    "# Turn the images into a single multi-channel image.\n",
    "height = 28\n",
    "width = 28\n",
    "multi_channel_img = torch.tensor(np.zeros((training_data._num_channels, height, \n",
    "                                           width)), dtype=torch.float32)\n",
    "for idx, sample in enumerate(results):\n",
    "  multi_channel_img[idx] = torch.tensor(np.array(sample.image), dtype=torch.float32)\n",
    "\n",
    "# Make sure we don't accidentally mix classes in our results.\n",
    "  assert len(list(filter(lambda sample: sample.target != target_class, results))) == 0, \\\n",
    "         \"Classes are getting mixed up in dataset results!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed MNIST global mean over training data: 0.0846\n",
      "Computed MNIST std over training data: 0.1953\n"
     ]
    }
   ],
   "source": [
    "# TODO: See if setting num_workers > 1 or using pin_memory can help with CPU/GPU transfers.\n",
    "\n",
    "training_data = MultiChannelBrightnessMNIST(data_path, num_channels=num_channels,\n",
    "                                            train=True, download=True)\n",
    "\n",
    "print(\"Computed MNIST global mean over training data: {}\".format(training_data.global_mean))\n",
    "print(\"Computed MNIST std over training data: {}\".format(training_data.global_std))\n",
    "\n",
    "training_data.transform = transforms = torchvision.transforms.Compose([\n",
    "  torchvision.transforms.ToTensor(),\n",
    "  torchvision.transforms.Normalize((training_data.global_mean,), (training_data.global_std,))\n",
    "])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(training_data, batch_size=batch_size_train,\n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  MultiChannelBrightnessMNIST(data_path, num_channels=num_channels,\n",
    "                              train=False, download=True, transform=torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((training_data.global_mean,), (training_data.global_std,))\n",
    "  ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self, num_channels):\n",
    "    super(Net, self).__init__()\n",
    "    self.num_channels = num_channels\n",
    "\n",
    "    # TODO: Try this _without_ expanding the size of all the downstream channels\n",
    "    # by num_channels; perhaps we don't need this extra expanded capacity.\n",
    "    self.conv1 = nn.Conv2d(in_channels=num_channels, out_channels=10 * num_channels,\n",
    "                           kernel_size=5)\n",
    "    self.conv2 = nn.Conv2d(in_channels=10 * num_channels, out_channels=20 * num_channels,\n",
    "                           kernel_size=5)\n",
    "    self.conv2_drop = nn.Dropout2d()\n",
    "    self.fc1 = nn.Linear(in_features=320 * num_channels, out_features=50 * num_channels)\n",
    "    self.fc2 = nn.Linear(in_features=50 * num_channels, out_features=10)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2))\n",
    "    x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), kernel_size=2))\n",
    "    x = x.view(-1, 320 * self.num_channels)\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.dropout(x, training=self.training)\n",
    "    x = self.fc2(x)\n",
    "    return F.log_softmax(x)\n",
    "\n",
    "model = Net(num_channels)\n",
    "model.cuda()\n",
    "\n",
    "# Use a second-order optimizer like Adam so that we don't need to deal with things\n",
    "# like learning rates and momentum.\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "train_losses, train_counter, test_losses = [], [], []\n",
    "test_counter = [i * len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
    "\n",
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets, example_debug) = next(examples)\n",
    "\n",
    "def train(epoch):\n",
    "  model.train()\n",
    "  for batch_idx, (data, target, debug) in enumerate(train_loader):\n",
    "    data = data.to(device)\n",
    "    target = target.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    output = output.to(device)\n",
    "    loss = F.nll_loss(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if batch_idx % log_interval == 0:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "        100.0 * batch_idx / len(train_loader), loss.item()))\n",
    "      train_losses.append(loss.item())\n",
    "      train_counter.append(\n",
    "        (batch_idx * 64) + ((epoch - 1) * len(train_loader.dataset)))\n",
    "      torch.save(model.state_dict(), model_path)\n",
    "      torch.save(optimizer.state_dict(), optimizer_path)\n",
    "\n",
    "def test():\n",
    "  model.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data, target, debug in test_loader:\n",
    "      data = data.to(device)\n",
    "      target = target.to(device)\n",
    "      output = model(data)\n",
    "      output = output.to(device)\n",
    "      # TODO: Handle the deprecation warning for log_softmax needing an explicit 'dim' argument\n",
    "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "      pred = output.data.max(1, keepdim=True)[1]\n",
    "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "  test_loss /= len(test_loader.dataset)\n",
    "  test_losses.append(test_loss)\n",
    "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100.0 * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/gpfs_gl4_16mb/b9p111/fdl_sw/conda/envs/wmlce_py3_sdo/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.3093, Accuracy: 494/5000 (9%)\n",
      "\n",
      "Train Epoch: 1 [0/30000 (0%)]\tLoss: 2.355919\n",
      "Train Epoch: 1 [640/30000 (2%)]\tLoss: 2.155636\n",
      "Train Epoch: 1 [1280/30000 (4%)]\tLoss: 1.849903\n",
      "Train Epoch: 1 [1920/30000 (6%)]\tLoss: 1.127938\n",
      "Train Epoch: 1 [2560/30000 (9%)]\tLoss: 0.865701\n",
      "Train Epoch: 1 [3200/30000 (11%)]\tLoss: 0.473984\n",
      "Train Epoch: 1 [3840/30000 (13%)]\tLoss: 0.599755\n",
      "Train Epoch: 1 [4480/30000 (15%)]\tLoss: 0.463754\n",
      "Train Epoch: 1 [5120/30000 (17%)]\tLoss: 0.396319\n",
      "Train Epoch: 1 [5760/30000 (19%)]\tLoss: 0.374005\n",
      "Train Epoch: 1 [6400/30000 (21%)]\tLoss: 0.472269\n",
      "Train Epoch: 1 [7040/30000 (23%)]\tLoss: 0.441030\n",
      "Train Epoch: 1 [7680/30000 (26%)]\tLoss: 0.166904\n",
      "Train Epoch: 1 [8320/30000 (28%)]\tLoss: 0.314032\n",
      "Train Epoch: 1 [8960/30000 (30%)]\tLoss: 0.342033\n",
      "Train Epoch: 1 [9600/30000 (32%)]\tLoss: 0.244782\n",
      "Train Epoch: 1 [10240/30000 (34%)]\tLoss: 0.300849\n",
      "Train Epoch: 1 [10880/30000 (36%)]\tLoss: 0.073235\n",
      "Train Epoch: 1 [11520/30000 (38%)]\tLoss: 0.150209\n",
      "Train Epoch: 1 [12160/30000 (41%)]\tLoss: 0.201273\n",
      "Train Epoch: 1 [12800/30000 (43%)]\tLoss: 0.231987\n",
      "Train Epoch: 1 [13440/30000 (45%)]\tLoss: 0.272293\n",
      "Train Epoch: 1 [14080/30000 (47%)]\tLoss: 0.080461\n",
      "Train Epoch: 1 [14720/30000 (49%)]\tLoss: 0.190297\n",
      "Train Epoch: 1 [15360/30000 (51%)]\tLoss: 0.315496\n",
      "Train Epoch: 1 [16000/30000 (53%)]\tLoss: 0.235896\n",
      "Train Epoch: 1 [16640/30000 (55%)]\tLoss: 0.100833\n",
      "Train Epoch: 1 [17280/30000 (58%)]\tLoss: 0.316804\n",
      "Train Epoch: 1 [17920/30000 (60%)]\tLoss: 0.278872\n",
      "Train Epoch: 1 [18560/30000 (62%)]\tLoss: 0.287791\n",
      "Train Epoch: 1 [19200/30000 (64%)]\tLoss: 0.149853\n",
      "Train Epoch: 1 [19840/30000 (66%)]\tLoss: 0.143356\n",
      "Train Epoch: 1 [20480/30000 (68%)]\tLoss: 0.273319\n",
      "Train Epoch: 1 [21120/30000 (70%)]\tLoss: 0.197491\n",
      "Train Epoch: 1 [21760/30000 (72%)]\tLoss: 0.078056\n",
      "Train Epoch: 1 [22400/30000 (75%)]\tLoss: 0.239046\n",
      "Train Epoch: 1 [23040/30000 (77%)]\tLoss: 0.192007\n",
      "Train Epoch: 1 [23680/30000 (79%)]\tLoss: 0.163993\n",
      "Train Epoch: 1 [24320/30000 (81%)]\tLoss: 0.185922\n",
      "Train Epoch: 1 [24960/30000 (83%)]\tLoss: 0.091506\n",
      "Train Epoch: 1 [25600/30000 (85%)]\tLoss: 0.143108\n",
      "Train Epoch: 1 [26240/30000 (87%)]\tLoss: 0.032231\n",
      "Train Epoch: 1 [26880/30000 (90%)]\tLoss: 0.202292\n",
      "Train Epoch: 1 [27520/30000 (92%)]\tLoss: 0.063539\n",
      "Train Epoch: 1 [28160/30000 (94%)]\tLoss: 0.231241\n",
      "Train Epoch: 1 [28800/30000 (96%)]\tLoss: 0.203328\n",
      "Train Epoch: 1 [29440/30000 (98%)]\tLoss: 0.105724\n",
      "\n",
      "Test set: Avg. loss: 0.0479, Accuracy: 4937/5000 (98%)\n",
      "\n",
      "Train Epoch: 2 [0/30000 (0%)]\tLoss: 0.180654\n",
      "Train Epoch: 2 [640/30000 (2%)]\tLoss: 0.207531\n",
      "Train Epoch: 2 [1280/30000 (4%)]\tLoss: 0.106161\n",
      "Train Epoch: 2 [1920/30000 (6%)]\tLoss: 0.026620\n",
      "Train Epoch: 2 [2560/30000 (9%)]\tLoss: 0.019223\n",
      "Train Epoch: 2 [3200/30000 (11%)]\tLoss: 0.360185\n",
      "Train Epoch: 2 [3840/30000 (13%)]\tLoss: 0.021476\n",
      "Train Epoch: 2 [4480/30000 (15%)]\tLoss: 0.142792\n",
      "Train Epoch: 2 [5120/30000 (17%)]\tLoss: 0.074623\n",
      "Train Epoch: 2 [5760/30000 (19%)]\tLoss: 0.063868\n",
      "Train Epoch: 2 [6400/30000 (21%)]\tLoss: 0.095092\n",
      "Train Epoch: 2 [7040/30000 (23%)]\tLoss: 0.125548\n",
      "Train Epoch: 2 [7680/30000 (26%)]\tLoss: 0.160841\n",
      "Train Epoch: 2 [8320/30000 (28%)]\tLoss: 0.076622\n",
      "Train Epoch: 2 [8960/30000 (30%)]\tLoss: 0.173191\n",
      "Train Epoch: 2 [9600/30000 (32%)]\tLoss: 0.023563\n",
      "Train Epoch: 2 [10240/30000 (34%)]\tLoss: 0.228055\n",
      "Train Epoch: 2 [10880/30000 (36%)]\tLoss: 0.169555\n",
      "Train Epoch: 2 [11520/30000 (38%)]\tLoss: 0.078230\n",
      "Train Epoch: 2 [12160/30000 (41%)]\tLoss: 0.109727\n",
      "Train Epoch: 2 [12800/30000 (43%)]\tLoss: 0.088328\n",
      "Train Epoch: 2 [13440/30000 (45%)]\tLoss: 0.086179\n",
      "Train Epoch: 2 [14080/30000 (47%)]\tLoss: 0.075977\n",
      "Train Epoch: 2 [14720/30000 (49%)]\tLoss: 0.027497\n",
      "Train Epoch: 2 [15360/30000 (51%)]\tLoss: 0.162528\n",
      "Train Epoch: 2 [16000/30000 (53%)]\tLoss: 0.123733\n",
      "Train Epoch: 2 [16640/30000 (55%)]\tLoss: 0.031499\n",
      "Train Epoch: 2 [17280/30000 (58%)]\tLoss: 0.039010\n",
      "Train Epoch: 2 [17920/30000 (60%)]\tLoss: 0.221787\n",
      "Train Epoch: 2 [18560/30000 (62%)]\tLoss: 0.066476\n",
      "Train Epoch: 2 [19200/30000 (64%)]\tLoss: 0.030876\n",
      "Train Epoch: 2 [19840/30000 (66%)]\tLoss: 0.029868\n",
      "Train Epoch: 2 [20480/30000 (68%)]\tLoss: 0.063243\n",
      "Train Epoch: 2 [21120/30000 (70%)]\tLoss: 0.235884\n",
      "Train Epoch: 2 [21760/30000 (72%)]\tLoss: 0.019733\n",
      "Train Epoch: 2 [22400/30000 (75%)]\tLoss: 0.143682\n",
      "Train Epoch: 2 [23040/30000 (77%)]\tLoss: 0.083519\n",
      "Train Epoch: 2 [23680/30000 (79%)]\tLoss: 0.041655\n",
      "Train Epoch: 2 [24320/30000 (81%)]\tLoss: 0.162674\n",
      "Train Epoch: 2 [24960/30000 (83%)]\tLoss: 0.070324\n",
      "Train Epoch: 2 [25600/30000 (85%)]\tLoss: 0.084336\n",
      "Train Epoch: 2 [26240/30000 (87%)]\tLoss: 0.217536\n",
      "Train Epoch: 2 [26880/30000 (90%)]\tLoss: 0.074624\n",
      "Train Epoch: 2 [27520/30000 (92%)]\tLoss: 0.049083\n",
      "Train Epoch: 2 [28160/30000 (94%)]\tLoss: 0.066287\n",
      "Train Epoch: 2 [28800/30000 (96%)]\tLoss: 0.097386\n",
      "Train Epoch: 2 [29440/30000 (98%)]\tLoss: 0.063440\n",
      "\n",
      "Test set: Avg. loss: 0.0244, Accuracy: 4962/5000 (99%)\n",
      "\n",
      "Train Epoch: 3 [0/30000 (0%)]\tLoss: 0.043603\n",
      "Train Epoch: 3 [640/30000 (2%)]\tLoss: 0.137297\n",
      "Train Epoch: 3 [1280/30000 (4%)]\tLoss: 0.116079\n",
      "Train Epoch: 3 [1920/30000 (6%)]\tLoss: 0.220815\n",
      "Train Epoch: 3 [2560/30000 (9%)]\tLoss: 0.112140\n",
      "Train Epoch: 3 [3200/30000 (11%)]\tLoss: 0.043036\n",
      "Train Epoch: 3 [3840/30000 (13%)]\tLoss: 0.049603\n",
      "Train Epoch: 3 [4480/30000 (15%)]\tLoss: 0.065789\n",
      "Train Epoch: 3 [5120/30000 (17%)]\tLoss: 0.057031\n",
      "Train Epoch: 3 [5760/30000 (19%)]\tLoss: 0.024014\n",
      "Train Epoch: 3 [6400/30000 (21%)]\tLoss: 0.168974\n",
      "Train Epoch: 3 [7040/30000 (23%)]\tLoss: 0.085979\n",
      "Train Epoch: 3 [7680/30000 (26%)]\tLoss: 0.046854\n",
      "Train Epoch: 3 [8320/30000 (28%)]\tLoss: 0.114431\n",
      "Train Epoch: 3 [8960/30000 (30%)]\tLoss: 0.028757\n",
      "Train Epoch: 3 [9600/30000 (32%)]\tLoss: 0.057523\n",
      "Train Epoch: 3 [10240/30000 (34%)]\tLoss: 0.079220\n",
      "Train Epoch: 3 [10880/30000 (36%)]\tLoss: 0.061691\n",
      "Train Epoch: 3 [11520/30000 (38%)]\tLoss: 0.026196\n",
      "Train Epoch: 3 [12160/30000 (41%)]\tLoss: 0.079137\n",
      "Train Epoch: 3 [12800/30000 (43%)]\tLoss: 0.028783\n",
      "Train Epoch: 3 [13440/30000 (45%)]\tLoss: 0.111722\n",
      "Train Epoch: 3 [14080/30000 (47%)]\tLoss: 0.251603\n",
      "Train Epoch: 3 [14720/30000 (49%)]\tLoss: 0.102654\n",
      "Train Epoch: 3 [15360/30000 (51%)]\tLoss: 0.038691\n",
      "Train Epoch: 3 [16000/30000 (53%)]\tLoss: 0.029259\n",
      "Train Epoch: 3 [16640/30000 (55%)]\tLoss: 0.095133\n",
      "Train Epoch: 3 [17280/30000 (58%)]\tLoss: 0.061223\n",
      "Train Epoch: 3 [17920/30000 (60%)]\tLoss: 0.125369\n",
      "Train Epoch: 3 [18560/30000 (62%)]\tLoss: 0.023262\n",
      "Train Epoch: 3 [19200/30000 (64%)]\tLoss: 0.231882\n",
      "Train Epoch: 3 [19840/30000 (66%)]\tLoss: 0.114922\n",
      "Train Epoch: 3 [20480/30000 (68%)]\tLoss: 0.006569\n",
      "Train Epoch: 3 [21120/30000 (70%)]\tLoss: 0.033594\n",
      "Train Epoch: 3 [21760/30000 (72%)]\tLoss: 0.011495\n",
      "Train Epoch: 3 [22400/30000 (75%)]\tLoss: 0.043733\n",
      "Train Epoch: 3 [23040/30000 (77%)]\tLoss: 0.199896\n",
      "Train Epoch: 3 [23680/30000 (79%)]\tLoss: 0.182523\n",
      "Train Epoch: 3 [24320/30000 (81%)]\tLoss: 0.036683\n",
      "Train Epoch: 3 [24960/30000 (83%)]\tLoss: 0.028778\n",
      "Train Epoch: 3 [25600/30000 (85%)]\tLoss: 0.051276\n",
      "Train Epoch: 3 [26240/30000 (87%)]\tLoss: 0.023804\n",
      "Train Epoch: 3 [26880/30000 (90%)]\tLoss: 0.072237\n",
      "Train Epoch: 3 [27520/30000 (92%)]\tLoss: 0.023231\n",
      "Train Epoch: 3 [28160/30000 (94%)]\tLoss: 0.072365\n",
      "Train Epoch: 3 [28800/30000 (96%)]\tLoss: 0.038139\n",
      "Train Epoch: 3 [29440/30000 (98%)]\tLoss: 0.085212\n",
      "\n",
      "Test set: Avg. loss: 0.0201, Accuracy: 4973/5000 (99%)\n",
      "\n",
      "Train Epoch: 4 [0/30000 (0%)]\tLoss: 0.013243\n",
      "Train Epoch: 4 [640/30000 (2%)]\tLoss: 0.020091\n",
      "Train Epoch: 4 [1280/30000 (4%)]\tLoss: 0.081863\n",
      "Train Epoch: 4 [1920/30000 (6%)]\tLoss: 0.049781\n",
      "Train Epoch: 4 [2560/30000 (9%)]\tLoss: 0.144364\n",
      "Train Epoch: 4 [3200/30000 (11%)]\tLoss: 0.015587\n",
      "Train Epoch: 4 [3840/30000 (13%)]\tLoss: 0.042306\n",
      "Train Epoch: 4 [4480/30000 (15%)]\tLoss: 0.030843\n",
      "Train Epoch: 4 [5120/30000 (17%)]\tLoss: 0.031730\n",
      "Train Epoch: 4 [5760/30000 (19%)]\tLoss: 0.052390\n",
      "Train Epoch: 4 [6400/30000 (21%)]\tLoss: 0.161995\n",
      "Train Epoch: 4 [7040/30000 (23%)]\tLoss: 0.033212\n",
      "Train Epoch: 4 [7680/30000 (26%)]\tLoss: 0.073496\n",
      "Train Epoch: 4 [8320/30000 (28%)]\tLoss: 0.132618\n",
      "Train Epoch: 4 [8960/30000 (30%)]\tLoss: 0.083988\n",
      "Train Epoch: 4 [9600/30000 (32%)]\tLoss: 0.025043\n",
      "Train Epoch: 4 [10240/30000 (34%)]\tLoss: 0.082980\n",
      "Train Epoch: 4 [10880/30000 (36%)]\tLoss: 0.064382\n",
      "Train Epoch: 4 [11520/30000 (38%)]\tLoss: 0.103660\n",
      "Train Epoch: 4 [12160/30000 (41%)]\tLoss: 0.011856\n",
      "Train Epoch: 4 [12800/30000 (43%)]\tLoss: 0.077543\n",
      "Train Epoch: 4 [13440/30000 (45%)]\tLoss: 0.047558\n",
      "Train Epoch: 4 [14080/30000 (47%)]\tLoss: 0.048123\n",
      "Train Epoch: 4 [14720/30000 (49%)]\tLoss: 0.161836\n",
      "Train Epoch: 4 [15360/30000 (51%)]\tLoss: 0.060158\n",
      "Train Epoch: 4 [16000/30000 (53%)]\tLoss: 0.154614\n",
      "Train Epoch: 4 [16640/30000 (55%)]\tLoss: 0.021219\n",
      "Train Epoch: 4 [17280/30000 (58%)]\tLoss: 0.010189\n",
      "Train Epoch: 4 [17920/30000 (60%)]\tLoss: 0.257052\n",
      "Train Epoch: 4 [18560/30000 (62%)]\tLoss: 0.084257\n",
      "Train Epoch: 4 [19200/30000 (64%)]\tLoss: 0.014812\n",
      "Train Epoch: 4 [19840/30000 (66%)]\tLoss: 0.075032\n",
      "Train Epoch: 4 [20480/30000 (68%)]\tLoss: 0.023842\n",
      "Train Epoch: 4 [21120/30000 (70%)]\tLoss: 0.143924\n",
      "Train Epoch: 4 [21760/30000 (72%)]\tLoss: 0.068671\n",
      "Train Epoch: 4 [22400/30000 (75%)]\tLoss: 0.073809\n",
      "Train Epoch: 4 [23040/30000 (77%)]\tLoss: 0.126908\n",
      "Train Epoch: 4 [23680/30000 (79%)]\tLoss: 0.104862\n",
      "Train Epoch: 4 [24320/30000 (81%)]\tLoss: 0.081913\n",
      "Train Epoch: 4 [24960/30000 (83%)]\tLoss: 0.022673\n",
      "Train Epoch: 4 [25600/30000 (85%)]\tLoss: 0.040756\n",
      "Train Epoch: 4 [26240/30000 (87%)]\tLoss: 0.154979\n",
      "Train Epoch: 4 [26880/30000 (90%)]\tLoss: 0.015021\n",
      "Train Epoch: 4 [27520/30000 (92%)]\tLoss: 0.044953\n",
      "Train Epoch: 4 [28160/30000 (94%)]\tLoss: 0.037357\n",
      "Train Epoch: 4 [28800/30000 (96%)]\tLoss: 0.131019\n",
      "Train Epoch: 4 [29440/30000 (98%)]\tLoss: 0.082272\n",
      "\n",
      "Test set: Avg. loss: 0.0105, Accuracy: 4985/5000 (99%)\n",
      "\n",
      "Train Epoch: 5 [0/30000 (0%)]\tLoss: 0.074217\n",
      "Train Epoch: 5 [640/30000 (2%)]\tLoss: 0.044662\n",
      "Train Epoch: 5 [1280/30000 (4%)]\tLoss: 0.004330\n",
      "Train Epoch: 5 [1920/30000 (6%)]\tLoss: 0.094907\n",
      "Train Epoch: 5 [2560/30000 (9%)]\tLoss: 0.018191\n",
      "Train Epoch: 5 [3200/30000 (11%)]\tLoss: 0.002497\n",
      "Train Epoch: 5 [3840/30000 (13%)]\tLoss: 0.015581\n",
      "Train Epoch: 5 [4480/30000 (15%)]\tLoss: 0.075763\n",
      "Train Epoch: 5 [5120/30000 (17%)]\tLoss: 0.081951\n",
      "Train Epoch: 5 [5760/30000 (19%)]\tLoss: 0.097242\n",
      "Train Epoch: 5 [6400/30000 (21%)]\tLoss: 0.056167\n",
      "Train Epoch: 5 [7040/30000 (23%)]\tLoss: 0.064861\n",
      "Train Epoch: 5 [7680/30000 (26%)]\tLoss: 0.079210\n",
      "Train Epoch: 5 [8320/30000 (28%)]\tLoss: 0.325961\n",
      "Train Epoch: 5 [8960/30000 (30%)]\tLoss: 0.030630\n",
      "Train Epoch: 5 [9600/30000 (32%)]\tLoss: 0.163906\n",
      "Train Epoch: 5 [10240/30000 (34%)]\tLoss: 0.105145\n",
      "Train Epoch: 5 [10880/30000 (36%)]\tLoss: 0.015394\n",
      "Train Epoch: 5 [11520/30000 (38%)]\tLoss: 0.099436\n",
      "Train Epoch: 5 [12160/30000 (41%)]\tLoss: 0.004399\n",
      "Train Epoch: 5 [12800/30000 (43%)]\tLoss: 0.010959\n",
      "Train Epoch: 5 [13440/30000 (45%)]\tLoss: 0.061713\n",
      "Train Epoch: 5 [14080/30000 (47%)]\tLoss: 0.008426\n",
      "Train Epoch: 5 [14720/30000 (49%)]\tLoss: 0.024660\n",
      "Train Epoch: 5 [15360/30000 (51%)]\tLoss: 0.018924\n",
      "Train Epoch: 5 [16000/30000 (53%)]\tLoss: 0.185333\n",
      "Train Epoch: 5 [16640/30000 (55%)]\tLoss: 0.008093\n",
      "Train Epoch: 5 [17280/30000 (58%)]\tLoss: 0.015321\n",
      "Train Epoch: 5 [17920/30000 (60%)]\tLoss: 0.092561\n",
      "Train Epoch: 5 [18560/30000 (62%)]\tLoss: 0.024640\n",
      "Train Epoch: 5 [19200/30000 (64%)]\tLoss: 0.007941\n",
      "Train Epoch: 5 [19840/30000 (66%)]\tLoss: 0.006765\n",
      "Train Epoch: 5 [20480/30000 (68%)]\tLoss: 0.111338\n",
      "Train Epoch: 5 [21120/30000 (70%)]\tLoss: 0.012569\n",
      "Train Epoch: 5 [21760/30000 (72%)]\tLoss: 0.029929\n",
      "Train Epoch: 5 [22400/30000 (75%)]\tLoss: 0.019022\n",
      "Train Epoch: 5 [23040/30000 (77%)]\tLoss: 0.036946\n",
      "Train Epoch: 5 [23680/30000 (79%)]\tLoss: 0.207699\n",
      "Train Epoch: 5 [24320/30000 (81%)]\tLoss: 0.036174\n",
      "Train Epoch: 5 [24960/30000 (83%)]\tLoss: 0.012279\n",
      "Train Epoch: 5 [25600/30000 (85%)]\tLoss: 0.052897\n",
      "Train Epoch: 5 [26240/30000 (87%)]\tLoss: 0.003603\n",
      "Train Epoch: 5 [26880/30000 (90%)]\tLoss: 0.176693\n",
      "Train Epoch: 5 [27520/30000 (92%)]\tLoss: 0.017773\n",
      "Train Epoch: 5 [28160/30000 (94%)]\tLoss: 0.042338\n",
      "Train Epoch: 5 [28800/30000 (96%)]\tLoss: 0.025445\n",
      "Train Epoch: 5 [29440/30000 (98%)]\tLoss: 0.017418\n",
      "\n",
      "Test set: Avg. loss: 0.0176, Accuracy: 4974/5000 (99%)\n",
      "\n",
      "Train Epoch: 6 [0/30000 (0%)]\tLoss: 0.135952\n",
      "Train Epoch: 6 [640/30000 (2%)]\tLoss: 0.023014\n",
      "Train Epoch: 6 [1280/30000 (4%)]\tLoss: 0.024448\n",
      "Train Epoch: 6 [1920/30000 (6%)]\tLoss: 0.073254\n",
      "Train Epoch: 6 [2560/30000 (9%)]\tLoss: 0.053038\n",
      "Train Epoch: 6 [3200/30000 (11%)]\tLoss: 0.033170\n",
      "Train Epoch: 6 [3840/30000 (13%)]\tLoss: 0.006222\n",
      "Train Epoch: 6 [4480/30000 (15%)]\tLoss: 0.012708\n",
      "Train Epoch: 6 [5120/30000 (17%)]\tLoss: 0.001904\n",
      "Train Epoch: 6 [5760/30000 (19%)]\tLoss: 0.046308\n",
      "Train Epoch: 6 [6400/30000 (21%)]\tLoss: 0.009110\n",
      "Train Epoch: 6 [7040/30000 (23%)]\tLoss: 0.026581\n",
      "Train Epoch: 6 [7680/30000 (26%)]\tLoss: 0.006995\n",
      "Train Epoch: 6 [8320/30000 (28%)]\tLoss: 0.023463\n",
      "Train Epoch: 6 [8960/30000 (30%)]\tLoss: 0.032769\n",
      "Train Epoch: 6 [9600/30000 (32%)]\tLoss: 0.080842\n",
      "Train Epoch: 6 [10240/30000 (34%)]\tLoss: 0.043140\n",
      "Train Epoch: 6 [10880/30000 (36%)]\tLoss: 0.038634\n",
      "Train Epoch: 6 [11520/30000 (38%)]\tLoss: 0.026031\n",
      "Train Epoch: 6 [12160/30000 (41%)]\tLoss: 0.027349\n",
      "Train Epoch: 6 [12800/30000 (43%)]\tLoss: 0.032320\n",
      "Train Epoch: 6 [13440/30000 (45%)]\tLoss: 0.037447\n",
      "Train Epoch: 6 [14080/30000 (47%)]\tLoss: 0.072317\n",
      "Train Epoch: 6 [14720/30000 (49%)]\tLoss: 0.007343\n",
      "Train Epoch: 6 [15360/30000 (51%)]\tLoss: 0.010180\n",
      "Train Epoch: 6 [16000/30000 (53%)]\tLoss: 0.052454\n",
      "Train Epoch: 6 [16640/30000 (55%)]\tLoss: 0.030002\n",
      "Train Epoch: 6 [17280/30000 (58%)]\tLoss: 0.045073\n",
      "Train Epoch: 6 [17920/30000 (60%)]\tLoss: 0.017795\n",
      "Train Epoch: 6 [18560/30000 (62%)]\tLoss: 0.018213\n",
      "Train Epoch: 6 [19200/30000 (64%)]\tLoss: 0.026272\n",
      "Train Epoch: 6 [19840/30000 (66%)]\tLoss: 0.060897\n",
      "Train Epoch: 6 [20480/30000 (68%)]\tLoss: 0.079853\n",
      "Train Epoch: 6 [21120/30000 (70%)]\tLoss: 0.047113\n",
      "Train Epoch: 6 [21760/30000 (72%)]\tLoss: 0.162438\n",
      "Train Epoch: 6 [22400/30000 (75%)]\tLoss: 0.066208\n",
      "Train Epoch: 6 [23040/30000 (77%)]\tLoss: 0.024630\n",
      "Train Epoch: 6 [23680/30000 (79%)]\tLoss: 0.116073\n",
      "Train Epoch: 6 [24320/30000 (81%)]\tLoss: 0.052549\n",
      "Train Epoch: 6 [24960/30000 (83%)]\tLoss: 0.092250\n",
      "Train Epoch: 6 [25600/30000 (85%)]\tLoss: 0.073193\n",
      "Train Epoch: 6 [26240/30000 (87%)]\tLoss: 0.016017\n",
      "Train Epoch: 6 [26880/30000 (90%)]\tLoss: 0.098000\n",
      "Train Epoch: 6 [27520/30000 (92%)]\tLoss: 0.007652\n",
      "Train Epoch: 6 [28160/30000 (94%)]\tLoss: 0.040278\n",
      "Train Epoch: 6 [28800/30000 (96%)]\tLoss: 0.114863\n",
      "Train Epoch: 6 [29440/30000 (98%)]\tLoss: 0.076510\n",
      "\n",
      "Test set: Avg. loss: 0.0094, Accuracy: 4982/5000 (99%)\n",
      "\n",
      "Train Epoch: 7 [0/30000 (0%)]\tLoss: 0.007735\n",
      "Train Epoch: 7 [640/30000 (2%)]\tLoss: 0.040180\n",
      "Train Epoch: 7 [1280/30000 (4%)]\tLoss: 0.038923\n",
      "Train Epoch: 7 [1920/30000 (6%)]\tLoss: 0.003397\n",
      "Train Epoch: 7 [2560/30000 (9%)]\tLoss: 0.006052\n",
      "Train Epoch: 7 [3200/30000 (11%)]\tLoss: 0.069681\n",
      "Train Epoch: 7 [3840/30000 (13%)]\tLoss: 0.067627\n",
      "Train Epoch: 7 [4480/30000 (15%)]\tLoss: 0.145627\n",
      "Train Epoch: 7 [5120/30000 (17%)]\tLoss: 0.017645\n",
      "Train Epoch: 7 [5760/30000 (19%)]\tLoss: 0.049882\n",
      "Train Epoch: 7 [6400/30000 (21%)]\tLoss: 0.005209\n",
      "Train Epoch: 7 [7040/30000 (23%)]\tLoss: 0.054192\n",
      "Train Epoch: 7 [7680/30000 (26%)]\tLoss: 0.054319\n",
      "Train Epoch: 7 [8320/30000 (28%)]\tLoss: 0.046238\n",
      "Train Epoch: 7 [8960/30000 (30%)]\tLoss: 0.019328\n",
      "Train Epoch: 7 [9600/30000 (32%)]\tLoss: 0.051475\n",
      "Train Epoch: 7 [10240/30000 (34%)]\tLoss: 0.010180\n",
      "Train Epoch: 7 [10880/30000 (36%)]\tLoss: 0.006611\n",
      "Train Epoch: 7 [11520/30000 (38%)]\tLoss: 0.019321\n",
      "Train Epoch: 7 [12160/30000 (41%)]\tLoss: 0.098130\n",
      "Train Epoch: 7 [12800/30000 (43%)]\tLoss: 0.019182\n",
      "Train Epoch: 7 [13440/30000 (45%)]\tLoss: 0.025077\n",
      "Train Epoch: 7 [14080/30000 (47%)]\tLoss: 0.003039\n",
      "Train Epoch: 7 [14720/30000 (49%)]\tLoss: 0.060219\n",
      "Train Epoch: 7 [15360/30000 (51%)]\tLoss: 0.025335\n",
      "Train Epoch: 7 [16000/30000 (53%)]\tLoss: 0.040450\n",
      "Train Epoch: 7 [16640/30000 (55%)]\tLoss: 0.017771\n",
      "Train Epoch: 7 [17280/30000 (58%)]\tLoss: 0.003870\n",
      "Train Epoch: 7 [17920/30000 (60%)]\tLoss: 0.022079\n",
      "Train Epoch: 7 [18560/30000 (62%)]\tLoss: 0.009541\n",
      "Train Epoch: 7 [19200/30000 (64%)]\tLoss: 0.032657\n",
      "Train Epoch: 7 [19840/30000 (66%)]\tLoss: 0.014956\n",
      "Train Epoch: 7 [20480/30000 (68%)]\tLoss: 0.009504\n",
      "Train Epoch: 7 [21120/30000 (70%)]\tLoss: 0.026568\n",
      "Train Epoch: 7 [21760/30000 (72%)]\tLoss: 0.010583\n",
      "Train Epoch: 7 [22400/30000 (75%)]\tLoss: 0.017376\n",
      "Train Epoch: 7 [23040/30000 (77%)]\tLoss: 0.003092\n",
      "Train Epoch: 7 [23680/30000 (79%)]\tLoss: 0.018075\n",
      "Train Epoch: 7 [24320/30000 (81%)]\tLoss: 0.056325\n",
      "Train Epoch: 7 [24960/30000 (83%)]\tLoss: 0.002130\n",
      "Train Epoch: 7 [25600/30000 (85%)]\tLoss: 0.016977\n",
      "Train Epoch: 7 [26240/30000 (87%)]\tLoss: 0.019125\n",
      "Train Epoch: 7 [26880/30000 (90%)]\tLoss: 0.022152\n",
      "Train Epoch: 7 [27520/30000 (92%)]\tLoss: 0.022516\n",
      "Train Epoch: 7 [28160/30000 (94%)]\tLoss: 0.041995\n",
      "Train Epoch: 7 [28800/30000 (96%)]\tLoss: 0.035283\n",
      "Train Epoch: 7 [29440/30000 (98%)]\tLoss: 0.012233\n",
      "\n",
      "Test set: Avg. loss: 0.0087, Accuracy: 4986/5000 (99%)\n",
      "\n",
      "Train Epoch: 8 [0/30000 (0%)]\tLoss: 0.180425\n",
      "Train Epoch: 8 [640/30000 (2%)]\tLoss: 0.031651\n",
      "Train Epoch: 8 [1280/30000 (4%)]\tLoss: 0.008986\n",
      "Train Epoch: 8 [1920/30000 (6%)]\tLoss: 0.038260\n",
      "Train Epoch: 8 [2560/30000 (9%)]\tLoss: 0.103971\n",
      "Train Epoch: 8 [3200/30000 (11%)]\tLoss: 0.011077\n",
      "Train Epoch: 8 [3840/30000 (13%)]\tLoss: 0.011202\n",
      "Train Epoch: 8 [4480/30000 (15%)]\tLoss: 0.002323\n",
      "Train Epoch: 8 [5120/30000 (17%)]\tLoss: 0.005948\n",
      "Train Epoch: 8 [5760/30000 (19%)]\tLoss: 0.010560\n",
      "Train Epoch: 8 [6400/30000 (21%)]\tLoss: 0.030072\n",
      "Train Epoch: 8 [7040/30000 (23%)]\tLoss: 0.001166\n",
      "Train Epoch: 8 [7680/30000 (26%)]\tLoss: 0.035774\n",
      "Train Epoch: 8 [8320/30000 (28%)]\tLoss: 0.046409\n",
      "Train Epoch: 8 [8960/30000 (30%)]\tLoss: 0.008291\n",
      "Train Epoch: 8 [9600/30000 (32%)]\tLoss: 0.012594\n",
      "Train Epoch: 8 [10240/30000 (34%)]\tLoss: 0.005831\n",
      "Train Epoch: 8 [10880/30000 (36%)]\tLoss: 0.050254\n",
      "Train Epoch: 8 [11520/30000 (38%)]\tLoss: 0.087488\n",
      "Train Epoch: 8 [12160/30000 (41%)]\tLoss: 0.010195\n",
      "Train Epoch: 8 [12800/30000 (43%)]\tLoss: 0.026677\n",
      "Train Epoch: 8 [13440/30000 (45%)]\tLoss: 0.140707\n",
      "Train Epoch: 8 [14080/30000 (47%)]\tLoss: 0.009256\n",
      "Train Epoch: 8 [14720/30000 (49%)]\tLoss: 0.024776\n",
      "Train Epoch: 8 [15360/30000 (51%)]\tLoss: 0.100180\n",
      "Train Epoch: 8 [16000/30000 (53%)]\tLoss: 0.016508\n",
      "Train Epoch: 8 [16640/30000 (55%)]\tLoss: 0.018369\n",
      "Train Epoch: 8 [17280/30000 (58%)]\tLoss: 0.012811\n",
      "Train Epoch: 8 [17920/30000 (60%)]\tLoss: 0.075107\n",
      "Train Epoch: 8 [18560/30000 (62%)]\tLoss: 0.016588\n",
      "Train Epoch: 8 [19200/30000 (64%)]\tLoss: 0.005094\n",
      "Train Epoch: 8 [19840/30000 (66%)]\tLoss: 0.000812\n",
      "Train Epoch: 8 [20480/30000 (68%)]\tLoss: 0.044090\n",
      "Train Epoch: 8 [21120/30000 (70%)]\tLoss: 0.130700\n",
      "Train Epoch: 8 [21760/30000 (72%)]\tLoss: 0.024594\n",
      "Train Epoch: 8 [22400/30000 (75%)]\tLoss: 0.006773\n",
      "Train Epoch: 8 [23040/30000 (77%)]\tLoss: 0.015653\n",
      "Train Epoch: 8 [23680/30000 (79%)]\tLoss: 0.047850\n",
      "Train Epoch: 8 [24320/30000 (81%)]\tLoss: 0.346558\n",
      "Train Epoch: 8 [24960/30000 (83%)]\tLoss: 0.013739\n",
      "Train Epoch: 8 [25600/30000 (85%)]\tLoss: 0.127990\n",
      "Train Epoch: 8 [26240/30000 (87%)]\tLoss: 0.025039\n",
      "Train Epoch: 8 [26880/30000 (90%)]\tLoss: 0.050173\n",
      "Train Epoch: 8 [27520/30000 (92%)]\tLoss: 0.008377\n",
      "Train Epoch: 8 [28160/30000 (94%)]\tLoss: 0.030672\n",
      "Train Epoch: 8 [28800/30000 (96%)]\tLoss: 0.178358\n",
      "Train Epoch: 8 [29440/30000 (98%)]\tLoss: 0.037941\n",
      "\n",
      "Test set: Avg. loss: 0.0081, Accuracy: 4984/5000 (99%)\n",
      "\n",
      "CPU times: user 3min 56s, sys: 1.12 s, total: 3min 57s\n",
      "Wall time: 4min 3s\n"
     ]
    }
   ],
   "source": [
    "# To ensure reproducibility, ensure we never attempt to train without also reseting the random seed every time.\n",
    "if training_runs_count >= seed_reset_count:\n",
    "  msg = \"You didn't reset the random seed! Runs won't be reproducible. Re-run reseting random seed.\"\n",
    "  raise Exception(msg)\n",
    "\n",
    "training_runs_count += 1\n",
    "\n",
    "def time_wrapper():\n",
    "  test()\n",
    "  for epoch in range(1, n_epochs + 1):\n",
    "    train(epoch)\n",
    "    test()\n",
    "\n",
    "%time time_wrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Best Results\n",
    "\n",
    "<b>04b_simple_toy1: Test set: Avg. loss: 0.0081, Accuracy: 4984/5000 (99%)</b><br/>\n",
    "<b>03b_simple_toy1: Test set: Avg. loss: 0.0440, Accuracy: 9872/10000 (98%)</b><br/>\n",
    "<b>02b_simple_toy1: Test set: Avg. loss: 0.0393, Accuracy: 9870/10000 (98%)</b><br/>\n",
    "<b>01b_simple_toy1: Test set: Avg. loss: 0.0569, Accuracy: 9818/10000 (98%)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'negative log likelihood loss')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(train_counter, train_losses, color='blue')\n",
    "plt.scatter(test_counter, test_losses, color='red')\n",
    "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "plt.xlabel('number of training examples seen')\n",
    "plt.ylabel('negative log likelihood loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Example With Trained Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/gpfs_gl4_16mb/b9p111/fdl_sw/conda/envs/wmlce_py3_sdo/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  output = model(example_data.to(device))\n",
    "  output = output.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "  plt.subplot(2,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Prediction: {}\".format(\n",
    "    output.data.max(1, keepdim=True)[1][i].item()))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue Training From Saved Checkpoint &amp; Final Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continued_network = Net()\n",
    "# continued_optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "#                                 momentum=momentum)\n",
    "\n",
    "# network_state_dict = torch.load(model_path)\n",
    "# continued_network.load_state_dict(network_state_dict)\n",
    "\n",
    "# optimizer_state_dict = torch.load(optimizer_path)\n",
    "# continued_optimizer.load_state_dict(optimizer_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def continue_training():\n",
    "#   for i in range(4, 9):\n",
    "#     test_counter.append(i*len(train_loader.dataset))\n",
    "#     train(i)\n",
    "#     test()\n",
    "\n",
    "# %time continue_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure()\n",
    "# plt.plot(train_counter, train_losses, color='blue')\n",
    "# plt.scatter(test_counter, test_losses, color='red')\n",
    "# plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "# plt.xlabel('number of training examples seen')\n",
    "# plt.ylabel('negative log likelihood loss')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
