{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Standard Candle\" Toy 7: Regress Brightness AND Class Prediction\n",
    "\n",
    "Put the MNIST class prediction back in along with the brightness regressions.\n",
    "\n",
    "ABANDONED: Note, this work was never finished and doesn't work, since we took a different direction. Just checked in for reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this notebook should be run from the `notebooks/` subdirectory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Change this to what the notebook name is for each experiment to ensure\n",
    "# training results are saved into the right sub-directory.\n",
    "notebook_name = '07b_simple_toy1'\n",
    "\n",
    "from collections import namedtuple, defaultdict\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import shutil\n",
    "import pdb\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Protective code to ensure we always reset the random seed when doing training, or else\n",
    "# code won't be reproducible.\n",
    "if 'training_runs_count' not in globals():\n",
    "  training_runs_count = 0\n",
    "if 'seed_reset_count' not in globals():\n",
    "  seed_reset_count = 0\n",
    "\n",
    "# Path to data and training results.\n",
    "root_path = '../..' # Relative to: notebooks/01b_simple_toy\n",
    "data_path = os.path.join(root_path, 'data', notebook_name)\n",
    "results_path = os.path.join(root_path, 'training_results', notebook_name)\n",
    "model_path = os.path.join(results_path, 'model.pth')\n",
    "optimizer_path = os.path.join(results_path, 'optimizer.pth')\n",
    "\n",
    "for path in [data_path, results_path]:\n",
    "  if not os.path.exists(path):\n",
    "    print('{} does not exist; creating directory...'.format(os.path.abspath(path)))\n",
    "    os.makedirs(path)\n",
    "\n",
    "n_epochs = 8\n",
    "n_mnist_classes = 10\n",
    "# TODO: Since we have a larger GPU with more memory, explore a larger batch size for\n",
    "# training; this might speed up training but have an adverse effect on accuracy. Explore.\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "log_interval = 50\n",
    "\n",
    "# Number of MNIST images given to the network at once.\n",
    "num_channels = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:1 for training, current device: 0, total devices: 6\n"
     ]
    }
   ],
   "source": [
    "# Use the GPU.\n",
    "cuda_device = 1\n",
    "torch.backends.cudnn.enabled = True\n",
    "if not torch.cuda.is_available():\n",
    "  raise RuntimeError(\"CUDA not available! Unable to continue\")\n",
    "# Force ourselves to use only one GPU.\n",
    "device = torch.device(\"cuda:{}\".format(cuda_device))\n",
    "print(\"Using device {} for training, current device: {}, total devices: {}\".format(\n",
    "  device, torch.cuda.current_device(), torch.cuda.device_count()))\n",
    "\n",
    "# Force runs to be deterministic and reproducible.\n",
    "random_seed = 1\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "# TODO: What kind of performance hit are we seeing due to forcing CUDNN to\n",
    "# be deterministic?\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ['PYTHONHASHSEED'] = str(random_seed)\n",
    "seed_reset_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Custom pytorch dataset that returns multi-channel MNIST results all of the same class (0 through 9) but\n",
    "with different hand-drawn images as each channel. Also applies a random brightness factor to each image.\n",
    "\"\"\"\n",
    "class MultiChannelBrightnessMNIST(torchvision.datasets.MNIST):\n",
    "  \n",
    "  SampleTuple = namedtuple(\"Sample\", \"image orig_image mnist_class target_brightness\")\n",
    "  \n",
    "  # TODO: Use dynamic arguments to super() to not have to hard code these params\n",
    "  # from torchvision.datsets.MNIST.\n",
    "  def __init__(self, root, num_channels=num_channels, apply_brightness=True,\n",
    "               train=True, transform=None, target_transform=None, download=False):\n",
    "    super(MultiChannelBrightnessMNIST, self).__init__(\n",
    "      root, train, transform, target_transform, download)\n",
    "\n",
    "    if not apply_brightness:\n",
    "      print('WARNING: We are not applying a brightness degradation to our images!')\n",
    "    \n",
    "    self._data_name = \"train\" if train else \"test\"\n",
    "    self._num_channels = num_channels\n",
    "    self._apply_brightness = apply_brightness\n",
    "    self._debug_images_printed = False\n",
    "    \n",
    "    self._setup_samples()\n",
    "    self._compute_statistics()\n",
    "\n",
    "  def _setup_samples(self):\n",
    "    \"\"\"\n",
    "    Re-organizes the data into tuples of the following form to make them easier to work with,\n",
    "    as well as to record what the random brightness is for correctly generating statistics\n",
    "    and ground truth information:\n",
    "\n",
    "    (image, orig_image, mnist_class, target_brightness)\n",
    "\n",
    "    Also creates a hashtable that goes from the target class to all available image tuples\n",
    "    with those classes so we can quickly fetch images of the same class together.\n",
    "    \n",
    "    Note: internally in our lookup tables we store everything as either a scalar integer\n",
    "    or a numpy array for the image. We do not store them as Torch tensors. The images\n",
    "    themselves are stored internally as float32's rather than uint8.\n",
    "    \"\"\"\n",
    "    self._available_samples = defaultdict(lambda: [])\n",
    "    self._all_samples = []\n",
    "    for (image, mnist_class) in zip(self.data, self.targets):\n",
    "      assert isinstance(image, torch.Tensor), \\\n",
    "        \"self.data should have images as torch.Tensors\"\n",
    "      assert isinstance(mnist_class, torch.Tensor), \\\n",
    "        \"self.data should have mnist_class targets as torch.Tensors\"\n",
    "      \n",
    "      # Internally work with everything as scalars for the target, or as numpy\n",
    "      # float32's to ensure no conversion errors between Torch/Numpy/PIL.Image\n",
    "      # or incorrect force quantitization when going from uint8 to float32s.\n",
    "      mnist_class = mnist_class.item()\n",
    "      image = image.data.numpy()\n",
    "      assert image.dtype == np.uint8, \"image should initially be uint8\"\n",
    "      image = image.astype(np.float32)      \n",
    "      \n",
    "      target_brightness = self._compute_brightness()\n",
    "\n",
    "      # Our loss function currently assumes that all brightness values are in the\n",
    "      # range [0.0, 1.0]. If this assumption is broken make sure we know so that\n",
    "      # in the future we can do extra work to 'squash' brightness values back\n",
    "      # down to [0.0, 1.0]\n",
    "      assert target_brightness >= 0.0 and target_brightness <= 1.0, \\\n",
    "        \"Brightness must be in the range [0.0, 1.0] for the loss function to work\"\n",
    "      \n",
    "      darker_image = self._darken_image(target_brightness, image)\n",
    "      \n",
    "      assert isinstance(mnist_class, int), \\\n",
    "        \"Converted mnist class must ultimately be a scalar Python integer\"\n",
    "      assert isinstance(target_brightness, float), \\\n",
    "        \"Converted target brightness must ultimately be a scalar Python float\"\n",
    "      assert image.dtype == np.float32, \\\n",
    "        \"At end of _setup_samples() images should be uniformly float32 types\"\n",
    "\n",
    "      sample = self.SampleTuple(image=darker_image, orig_image=image,\n",
    "                                mnist_class=mnist_class,\n",
    "                                target_brightness=target_brightness)\n",
    "\n",
    "      self._available_samples[mnist_class].append(sample)\n",
    "      self._all_samples.append(sample)\n",
    "      \n",
    "  def _compute_brightness(self):\n",
    "    if self._apply_brightness:\n",
    "      # Randomly compute what the brightness setting should be for each sample.\n",
    "      # NOTE: Below 30% brightness accuracy degrades vs. the experiment 2 baseline.\n",
    "      return np.round((0.3 - 1.0) * np.random.random_sample() + 1.0, decimals=2)\n",
    "    else:\n",
    "      return 1.0\n",
    "\n",
    "  def _darken_image(self, brightness, img):\n",
    "    return brightness * img\n",
    "  \n",
    "  def _compute_statistics(self):\n",
    "    \"\"\"\n",
    "    Compute global mean and standard deviation for data input normalization.\n",
    "    \"\"\"\n",
    "    max_pixel_value = 255.0\n",
    "    means = np.zeros((len(self._all_samples)), dtype=np.float32)\n",
    "    stds = np.zeros((len(self._all_samples)), dtype=np.float32)\n",
    "    for idx, sample in enumerate(self._all_samples):\n",
    "      np_img = sample.image\n",
    "      assert type(np_img).__module__ == np.__name__, \\\n",
    "        '_compute_statistics should be working with numpy images!'\n",
    "      means[idx] = np.round((np_img / max_pixel_value).mean(), decimals=4)\n",
    "      stds[idx] = np.round((np_img / max_pixel_value).std(), decimals=4)\n",
    "\n",
    "    self.global_mean = np.round(np.sum(means) / len(means), decimals=4)\n",
    "    self.global_std = np.round(np.sum(stds) / len(means), decimals=4)\n",
    "    \n",
    "    # Ensure our computed mean and std are within what we've seen historically;\n",
    "    # otherwise flag this as a potential bug.\n",
    "    assert self.global_mean >= 0.05 and self.global_mean <= 0.2, \\\n",
    "      \"Computed global mean is outside historical ranges seen: {}\".format(\n",
    "        self.global_mean)\n",
    "    assert self.global_std >= 0.1 and self.global_std <= 0.4, \\\n",
    "      \"Computed global std is outside historical ranges seen: {}\".format(\n",
    "        self.global_std)\n",
    "      \n",
    "  def __getitem__(self, index):\n",
    "    # NOTE: We return items in bunches of num_channels. We also essentially\n",
    "    # ignore the index and just return random bunches of the same class.\n",
    "\n",
    "    # Randomly choose a class label.\n",
    "    mnist_class = np.random.randint(low=0, high=10)\n",
    "    assert isinstance(mnist_class, int), \\\n",
    "      \"For __getitem__ mnist_class must be a Python int\"\n",
    "    samples_with_class = self._available_samples[mnist_class]\n",
    "\n",
    "    # Fetch num_channel samples that have this target class.\n",
    "    result_idxs = np.random.choice(len(samples_with_class), size=(self._num_channels,),\n",
    "                                   replace=False)\n",
    "    results = [samples_with_class[idx] for idx in result_idxs]\n",
    "\n",
    "    for sample in results:\n",
    "      assert type(sample.image).__module__ == np.__name__, \\\n",
    "        '__getitem__ should be dealing with numpy images as input!'\n",
    "      assert sample.mnist_class == mnist_class, \\\n",
    "        'All items returned from __getitem__ should have the same MNIST class'\n",
    "\n",
    "    # Turn the images into a single multi-channel image.\n",
    "    height = 28\n",
    "    width = 28\n",
    "    multi_channel_img = torch.Tensor(np.zeros((self._num_channels, height, width),\n",
    "                                              dtype=np.float32))\n",
    "    for idx, sample in enumerate(results):\n",
    "      single_channel_img = sample.image\n",
    "      if self.transform is not None:\n",
    "        single_channel_img = self.transform(single_channel_img)\n",
    "      multi_channel_img[idx] = single_channel_img\n",
    "\n",
    "    assert multi_channel_img.shape == (self._num_channels, height, width), \\\n",
    "      \"multi_channel_img must have the correct shape\"\n",
    "\n",
    "    for sample in results:\n",
    "      assert isinstance(sample.target_brightness, float), \\\n",
    "        \"__getitem__ should be dealing with a scalar Python type for the target brightness!\"\n",
    "\n",
    "    # TODO: We aren't yet actually correctly implementing self.target_transform since we\n",
    "    # don't need it currently.\n",
    "    assert self.target_transform is None, \"self.target_transform is not yet implemented\"\n",
    "    assert isinstance(sample.target_brightness, float), \\\n",
    "      \"sample.target_brightness must be a Python float\"\n",
    "    target_brightness = torch.tensor(np.array([sample.target_brightness for sample in results],\n",
    "                                              dtype=np.float32))\n",
    "    debug_results = results\n",
    "\n",
    "    if not self._debug_images_printed:\n",
    "      print(\"\\n\\n\\nDebug images from the same fetch for {} dataset; \"\n",
    "            \"make sure all are same number and are legible:\".format(self._data_name))\n",
    "      for c in range(self._num_channels):\n",
    "        display(Image.fromarray(multi_channel_img[c].data.cpu().numpy().astype(np.uint8),\n",
    "                                mode='L'))\n",
    "      self._debug_images_printed = True\n",
    "\n",
    "    assert isinstance(target_brightness, torch.Tensor), \\\n",
    "      \"Final target brightnesses from __getitem__ must be a torch.Tensor\"\n",
    "    assert isinstance(multi_channel_img, torch.Tensor), \\\n",
    "      \"Final multi channel image from __getitem__ must be a torch.Tensor\"\n",
    "\n",
    "    return multi_channel_img, target_brightness, debug_results\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed MNIST global mean over training data: 0.085\n",
      "Computed MNIST std over training data: 0.1959\n"
     ]
    }
   ],
   "source": [
    "# TODO: See if setting num_workers > 1 or using pin_memory can help with CPU/GPU transfers.\n",
    "# TODO: Something is slow in this cell; figure out what it is and speed it up.\n",
    "\n",
    "training_data = MultiChannelBrightnessMNIST(data_path, num_channels=num_channels,\n",
    "                                            train=True, download=True)\n",
    "\n",
    "print(\"Computed MNIST global mean over training data: {}\".format(training_data.global_mean))\n",
    "print(\"Computed MNIST std over training data: {}\".format(training_data.global_std))\n",
    "\n",
    "training_data.transform = transforms = torchvision.transforms.Compose([\n",
    "  torchvision.transforms.ToTensor(),\n",
    "  torchvision.transforms.Normalize((training_data.global_mean,), (training_data.global_std,))\n",
    "])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(training_data, batch_size=batch_size_train,\n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  MultiChannelBrightnessMNIST(data_path, num_channels=num_channels,\n",
    "                              train=False, download=True, transform=torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((training_data.global_mean,), (training_data.global_std,))\n",
    "  ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Debug images from the same fetch for train dataset; make sure all are same number and are legible:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA/klEQVR4nGNgGDRAYNMmBgYGFqxyM06/3IxTY+H1M3wMDAxM2OTm3/Bd9QlD1JiBgYGBQfTuhFRMHelrjRkYGBgq10VB+MjGBnUycjAwMNy+kv4GQ/Kd774DHxkYrgfY9+1CN1Nls6SWDgMD1+aQ7Rj2PVu5zpSPgYHhuGSZELoc84JNPMYMDAyiW5ciBGEhJHL6lPpZBgahIzc1EJIwB1nqvDzLwMCQfFWtksEaXfKIONeaNSy64gwcOQxz0Y2Nvq7IxLCKgYFh/x2tfnSdE18zMDAwMJjfjejLw/AlA4P5KqN1b6QwxSHAZr6+Ci45Bnn1bTjlxB/vwylHRwAAmeM++Ooi9a4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x2000FD0EF5F8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABCElEQVR4nM2PoWtCURjFfz5eXVCxTBwMQYwLFhk8rILBIVsWw4pgML1/QBgsbG0Mg2UqlonBYFAMhkUxCHsgsjR8wrb4BgMXrrv36QNBLDvlfvf84Dvng38mH0CsCPfztfN5gT6xAdABnCbUc+9r6sDEsAA04DwJlLuufdkCf/DqDSDtzdTkZHihDoj0D2FkHoFKXrZNOQBjB4DCVH001xYuh8NTMUXl2lkYIJDAMGmNpgBHslDiCeDEtutQmt9ttm3XbpXRi2+dUuk/SGOWUdAnnuSK10awpPwXFwz5wTprwfLH7FwveD52QSVxZ9b03Lmt/eDoewe0Bmr2FCLSrYa5+doVd6B+AQk8Pe/ylO9tAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x2001B3BE4710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABM0lEQVR4nM2RMUhCURSGP/OE8KAGKWgysEkqWuIFDjZYQ1vEa6uptqaGQGgrEClcjaIpBEMfJBVIiFNNEkG0ONTiUIPQIPEiukiL73kf2NbQP51zv/OfH86Ff6aAV+1/GR9pt7EXhoIw0JtTTnXVZWbmEx1mIZnv1lYOdDg+poWVIBwHcfvFEQ9d3D9C7DziOdOXIsZeCIDlgCFSjnhO22wrnDsAZnY7jkou9TKtHLC9Buw0wh0ohhpomVBZfwUOg3PAdAIfrFxFADYAmm9ocFB4CALx2IQDzTP/+VK0p1qFRrUmioPZOv4jMNz8PtrcAgrPXebCehHgJrEC12bUnXfX2vNZRAHS8b7GW2uNvmdaIiLSY1rmCcdKqegt/SAAp081+st+SZV9D7rTyjP5i+9P9QMVEVCRcbCMnwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x2000FD0EF5F8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA90lEQVR4nGNgGGSAEVPIWfUP80wsSs0YGBgYePT1oVwmJKl2/ggrBgYGD4YXRpj6tPSl1jIwyObo12PqFHBgETVnYMg6/n4GpqTh0T/J6QwMv/4kw1yJ5NqQ235bzzFIS/x59xhDp1kKg9k5BoajDKbWmM5ZbPycjYEhkcG8By6EMPZ8AoP053/X5I7eNoQJwY0N+M/A8PTbFzmGbLgcQvJkKgOD6WkGBobLmDYyPDXfx8Qg22V8Goscg3e8FQPD7R+Pm7BJQizR5+dB5qJIGjJUfcEleZbl4gtkPkpk67I94buNQ2fAV4YyZDkUyTtfNr3G6VYqAgCvmD2w0i8cpAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x2001B3BE4710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA/0lEQVR4nGNgGLLA7D2UoeXAwMDAwMCEJOdw5iyUxfcKXfLnJ97pUOZ5P1QjpW9lLpOVhnLYPzMwMDAwMMLkon9tvA9TuO7M8/lIxmqdfOgNl2PY/u8rsp0TppxyhcuFPIMymBgYGBi0dq0SguibupIh7JaiLsNMBgYGBgYWBgYG6V1uR/UYGBgYGPYvZXWUlbymy3AGbkx0wVwGBnZr3iOJy94yMDBYi2S/ZYC7VoQ1hoHN4OzqRxOyGRgYGKJl575G2Dklnpn57zx5nt/ZELHVUkj+t9OepFmE4HqwMuAE/FX/cEs6a8GZTOhyUxXtcGu0CEOwMXQKnkEXoS8AANUhPTW9izSCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x2000FD0EF5F8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA4UlEQVR4nGNgGEqAEYvYgl1ccxgYGBgYmDCk2AI/PHeCMFkwJLfdYTCOYsDUWWjvwaDtzbD8KAOmzrWH5bkYIkWWfzuN6SBLrtB/Wa03mW8cx+JIzhnB+xnMZqxDiCCMLZTfHe0olfQwDyEJd5DOZQ5DloeRDBa/1rrCxOB2ppgwMDAwMDDfMF0QmI6uk2X/fuOFDCsFTRl+/8PQycDAsGg/g1kmshOR/Mms+ZonnQEH0JlxEk0EEXwKXsfQowghuUTpjRlOSd1nWbgsZGAIY8UQgutU4JDFrdHjWSBuSZIBALlDMoG+BhbBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x2001B3BE4710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABD0lEQVR4nNWQvUoDYRBF7/qHLgqBoyQiIoFUgiI2orWNmmcI+ASBYGGjha1YCGqTRhGLpFKsDEIEbWRBiyWQQAoVQQsLbTQEFiwW5fvCbmPnNDPcM3dgrvT/a3t0UpK6TG1rai4cHto3WXs9c3DHriQ95drkFiyWOgdqktQDX/cmSp/Ow3OiIi0n+6Bg+XqB4FiSrgG3abL1OgQVSZo+hJ10qDphG/+UgiV5TbXGpODd+qEKAF42uQpuXqZzsTFxMvu7OlLvDOXI9/21bqBaHuxkYQweuDORSCq/kCjFsFsXNkzBCD5VHFBpONqXcR65KMYcvQT2bMn5Ga76VzTUeo307XvwcRZz9A3YjGF/qG9Whj9oq6F7ewAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x2000FD0EF5F8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Debug images from the same fetch for test dataset; make sure all are same number and are legible:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA9ElEQVR4nGNgGEBg3vuWB12MEUrf4kxn2Oe7WiDhv0QlumTfdFUGBga53VVrGTa4HEKT/B6MMEx+OozFBKG47yMkfzGgSf679oiBgWHLDQYGhp5z6JIMjNeePTO2TO1hYOg5D5dkgTH+XWC4wPDYmoHhBAOGTgjg7GBgeI9LkheFhyZZwcBwQw6XJAMDw7eTeCS13+GRZGe74Apjw70ilaB+dK0iAwODtWBsnS1EDBYrYgoiDAzOkSkMDAwMDAGrd6NIll1BMpp59w8UO7kWI0n+FUc1liH+3pW136dCOYhYg4KluZwMm455yW+LEAtF9wIdAQCavTsCZtOfFAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x20000B8B0160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAbElEQVR4nMWPwQmAMBAEF7ETQTvRl71oJ4HYi1iKcqlFf5JduPtmf8tkyRzQPpdVpRP4YPOXFixXagLnCALFhzeOqvWsw095uQOD/vMnWw6F/LxGh/KyUGMosgxPkVWhFEBiCicfjpHQktA8H3x8EgjmbXHXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x2000FD0EFB38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAArklEQVR4nGNgGAxAU8YUxmRBl/vZzn3xNJTNhC7pwo1gY0gKMJjswWWst/4xwRe4JDWFTq/HZexT8b8WOO18z2BwEMFjRJGzNZTdtx2HTt6Tsst4kPgoOm39GBxNkPjIOhu4GBgeIStG9koCN0MxB7Ikss44hleLfuGQNPvO8CqBAbtkw++wi0dQ5JDsrONmWP8VVRKuU+0Nw09JBhySYrYMl+/gkvzz9VUSwyAAAJ3VJmbpdQyiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x2000FD0EFB38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAjElEQVR4nGNgGATg0xs5OJsJQ7ZCBLfOnXvEcUuGmSPY6MZa8THglhRk4MJpKO+B2KM4df5domSNS5JJkPkrTjuFPP+q4pSUZviUiUtSVYuh8R8uyU18DLcYcEnu/6sTgVNyCcOh7ww4gNTZMEUUAUYk9rRz79eiSCIby45uFLLkOnyS17764nIO/QAA+UkcPdl2j/YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x2000FD0EFB38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAmUlEQVR4nGNgGGSAEc5K5ZFrlzmHIskEZ81mOi1fw4BDkqH3T+wTnJKYAFmSi4HhNU5Jk6ksTc9xSeY4/dFUxWnnpRCGN+FIfEZkSYZE0z+Ff3G5dj4Dgx8uYxkYuFhYcUs6/bHQZMAF/OQmXcCpc9NZZB568MkyvMAteZHFGLckw5/feCTx2cnAkBiORRkEiFVLceAzjBoAADPYHNf+NkhPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x2000FD0EFB38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAf0lEQVR4nGNgIBcwovDqf+45g8RlQpE89fcUA07JdGYm3JIn/qJwWZA5JqhKUXWe+odHJwM+nQxDSmcKmk4UyZlMF/HZaYzCRYnPJOk/V/SY/zIb+2PROec3g9GvX79+HTmDxbXnWP8wMLMyNDPMwaLTxIBhJst/FlbWTIYhCAD38SaTrvxT9QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x2000FD0EFB38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAtUlEQVR4nGNgoAcQb9E3wCm5SmNFDYLHgip5jEExAsFjQpEr3sIgwoBDko2HhUEZl425GhpXkfkoOlcyMHzAKfmEgaEZl+QEZYZzr3BJMnMzdJ7DJanIxFTFwMDAwCA4EyKAFAgKMv/2GDEwMDAwZJ7D0BkVwfAE4pM4TGPr4ayJmJIFDOgAISnowsCwBpfk1ysMDCm4JJXbGP7cxiV5PYnhjS+U7YfhoN/npkNZBpEYbhscAACzfCJ0zOmR3gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x2000FD0EFB38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_examples = enumerate(train_loader)\n",
    "train_batch_idx, (train_example_data, train_example_targets, train_example_debug) = next(train_examples)\n",
    "\n",
    "test_examples = enumerate(test_loader)\n",
    "test_batch_idx, (test_example_data, test_example_targets, test_example_debug) = next(test_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(output, target_brightnesses, target_mnist_classes, training):\n",
    "  \"\"\"\n",
    "  Returns our loss function, which includes terms to regress the brightness (given as alpha below) for each channel along with the class prediction term.\n",
    "  Each regression term is essentially an L1 norm loss function, with a small epsilon value to ensure we don't get an exact zero value, which will break\n",
    "  the log. Since the class prediction uses the negative log likelihood of the softmax of the correct predicted class, we also take the negative log\n",
    "  likelihood of each regressed L1 to ensure that each term gives values in a similar range. In addition, since the class-based softmax function has 1.0\n",
    "  indicating a \"good\" prediction and ~0.0 indicating a \"bad\" prediction, we want the regression terms to have a similar orientation; this requires us\n",
    "  to subtract the L1 loss from 1.0 to flip the values.\n",
    "\n",
    "  [$$]\n",
    "  L = \\sum_{c}^{\\text{\\# channels}} \\left[ -\\ln{\\left(1 - | \\alpha_{\\text{ground truth c}} - \\alpha_{\\text{prediction c}} + \\epsilon|\\right)} \\right]\n",
    "         + -\\ln{(\\text{Correct predicted class softmax})}\n",
    "  [/$$]\n",
    "  \"\"\"\n",
    "  # TODO!!!\n",
    "  class_preds = \n",
    "  F.nll_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self, num_channels):\n",
    "    super(Net, self).__init__()\n",
    "    self.num_channels = num_channels\n",
    "\n",
    "    # TODO: Try this _without_ expanding the size of all the downstream channels\n",
    "    # by num_channels; perhaps we don't need this extra expanded capacity.\n",
    "    self.conv1 = nn.Conv2d(in_channels=num_channels, out_channels=10 * num_channels,\n",
    "                           kernel_size=5)\n",
    "    self.conv2 = nn.Conv2d(in_channels=10 * num_channels, out_channels=20 * num_channels,\n",
    "                           kernel_size=5)\n",
    "    self.conv2_drop = nn.Dropout2d()\n",
    "    self.fc1 = nn.Linear(in_features=320 * num_channels, out_features=50 * num_channels)\n",
    "    self.fc2 = nn.Linear(in_features=50 * num_channels,\n",
    "                         out_features=num_channels + )\n",
    "    \n",
    "  def forward(self, x):\n",
    "    x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2))\n",
    "    x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), kernel_size=2))\n",
    "    x = x.view(-1, 320 * self.num_channels)\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.dropout(x, training=self.training)\n",
    "    x = F.relu(self.fc2(x))\n",
    "    \n",
    "    out_class, out_alpha = torch.split(x, (n_mnist_classes, n_channels), dim=0)\n",
    "    # Transform the final regressed brightness outputs to be between\n",
    "    # [0.0, 1.0] by using a sigmoid activation function.\n",
    "    out_alpha = F.sigmoid(out_alpha)\n",
    "    out_class = F.log_softmax(out_class)\n",
    "    \n",
    "    return out_class, out_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_accuracy_stats(correct, output, targets, num_column_labels, num_subsample):\n",
    "  # TODO: For efficiency reasons, convert all of this to torch rather than numpy operations\n",
    "  # so that we can do all this work on the GPU.\n",
    "  preds = output.cpu().numpy()\n",
    "  targets = targets.cpu().numpy()\n",
    "\n",
    "  # If a channel brightness prediction is within this percentage of the ground truth then we\n",
    "  # consider that prediction correct.\n",
    "  pct_close = 0.15\n",
    "\n",
    "  # Means across each row that collapses each batch entries predictions,\n",
    "  # the ground truth brightness, and the delta btw ground truth and prediction.\n",
    "  mean_per_channel_prediction = preds.mean(axis=0)\n",
    "  mean_per_channel_gt = targets.mean(axis=0)\n",
    "  per_channel_diff = np.abs(targets - preds)\n",
    "  mean_per_channel_diff = per_channel_diff.mean(axis=0)\n",
    "\n",
    "  # Various stats around channel correctness.\n",
    "  per_channel_correct = per_channel_diff <= np.abs(pct_close * targets)\n",
    "  # TODO: We can probably get rid of both of these np.where() conversions to 1/0s and just use the boolean\n",
    "  # array itself for the sum.\n",
    "  percentage_channels_correct = np.sum(np.where(per_channel_correct, 1, 0), axis=1, keepdims=True,\n",
    "                                       dtype=np.int)\n",
    "  correct_per_channel = np.sum(np.where(per_channel_correct, 1, 0), axis=1, keepdims=True,\n",
    "                               dtype=np.int)\n",
    "  pct_correct_per_channel = correct_per_channel / num_channels\n",
    "\n",
    "  # Which batch results have _all_ of their channel predictions fully correct?\n",
    "  # TODO: We can probably get rid of the np.where() conversion to 1/0s and just use the boolean\n",
    "  # array itself for the sum.\n",
    "  num_fully_correct_all_channels = np.where(pct_correct_per_channel == 1.0, 1, 0).sum()\n",
    "  pct_fully_correct_all_channels = num_fully_correct_all_channels / batch_size_test\n",
    "  correct += num_fully_correct_all_channels\n",
    "  \n",
    "  pretty_results = np.zeros((min(batch_size_test, len(preds)), num_column_labels), dtype=np.float32)\n",
    "\n",
    "  # The mean channel prediction across each row of the batch results.\n",
    "  pretty_results[:, 0] = np.round(preds.mean(axis=1), decimals=2)\n",
    "\n",
    "  # The mean channel ground truth across each row of the batch results.\n",
    "  pretty_results[:, 1] = np.round(targets.mean(axis=1), decimals=2)\n",
    "\n",
    "  # The mean difference btw prediction and grouth truth across each row of the batch results.\n",
    "  pretty_results[:, 2] = np.round(np.abs(targets - preds).mean(axis=1), decimals=2)\n",
    "\n",
    "  # Percentage correct across all the channels for a given batch row?\n",
    "  pretty_results[:, 3] = np.round(pct_correct_per_channel * 100.0, decimals=0)[:, 0].astype(np.int)\n",
    "  \n",
    "  # Randomly sub-sample some of the results since 100s or 1000s are too much to display.\n",
    "  lookup_idxs = np.random.choice(len(pretty_results), size=(num_subsample,))\n",
    "  pretty_results = pretty_results[lookup_idxs]\n",
    "\n",
    "  return correct, pretty_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(num_channels)\n",
    "model.cuda(device)\n",
    "\n",
    "# Use a second-order optimizer like Adam so that we don't need to deal with things\n",
    "# like learning rates and momentum.\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "def test(epoch):\n",
    "  print(\"\\n\\nTesting epoch {}\".format(epoch))\n",
    "  model.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  current_batch = 0\n",
    "  num_subsample = 5 # Per batch, how many results to subsample to print out for debugging.\n",
    "  column_labels = ['Pred', 'GT', 'Delta', 'Pct Correct']\n",
    "  pretty_results = np.zeros((int(math.ceil(len(test_loader.dataset) / batch_size_test) * num_subsample),\n",
    "                             len(column_labels)), dtype=np.float32)\n",
    "  with torch.no_grad():\n",
    "    for data, targets, debug in test_loader:\n",
    "      current_batch += 1\n",
    "      data = data.to(device)\n",
    "      targets = targets.to(device)\n",
    "      output = model(data)\n",
    "      output = output.to(device)\n",
    "      loss = F.l1_loss(output, targets)\n",
    "      test_loss += loss.item()\n",
    "\n",
    "      correct, pretty_print_subset = generate_train_accuracy_stats(\n",
    "        correct, output, targets, len(column_labels), num_subsample)\n",
    "      current_batch_idx = current_batch - 1\n",
    "      pretty_results[current_batch_idx*num_subsample:current_batch_idx*num_subsample+num_subsample] = pretty_print_subset\n",
    "\n",
    "  test_loss /= current_batch\n",
    "  test_losses.append(test_loss)\n",
    "  \n",
    "  print(\"\\n\\nRandom sample of mean predictions across channels for test set, \"\n",
    "        \"where each row is a test sample in the training batch:\\n\")\n",
    "  df = pandas.DataFrame(pretty_results, columns=column_labels)\n",
    "  print(df.to_string(index=False))\n",
    "  \n",
    "  print('\\n\\nTest set: Avg. loss: {:.8f}, Accuracy all channels correct: {}/{} ({:.0f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100.0 * (correct / len(test_loader.dataset))))\n",
    "\n",
    "def train(epoch):\n",
    "  # TODO: Training seems slow, what efforts can be done to speed it up?\n",
    "  print(\"\\n\\n===================================\\n\\n\")\n",
    "  print(\"\\n\\nTraining epoch {}\\n\".format(epoch))\n",
    "  model.train()\n",
    "  train_loss = 0\n",
    "  current_batch = 0\n",
    "  for batch_idx, (data, targets, debug) in enumerate(train_loader):\n",
    "    current_batch += 1\n",
    "    data = data.to(device)\n",
    "    targets = targets.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    out_class, out_alpha = model(data)\n",
    "    out_class = out_class.to(device)\n",
    "    out_alpha = out_alpha.to(device)\n",
    "    loss = compute_loss(out_class, out_alpha, , training=False)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss += loss.item()\n",
    "      \n",
    "    if batch_idx % log_interval == 0:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "        100.0 * (batch_idx / len(train_loader)), loss.item()))\n",
    "      torch.save(model.state_dict(), model_path)\n",
    "      torch.save(optimizer.state_dict(), optimizer_path)\n",
    "      \n",
    "  train_loss /= current_batch\n",
    "  train_losses.append(train_loss)\n",
    "    \n",
    "  print(\"\\nAt end of training epoch {}, Avg. loss is {:.8f}\".format(\n",
    "    epoch, train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Testing epoch 0\n",
      "\n",
      "\n",
      "Random sample of mean predictions across channels for test set, where each row is a test sample in the training batch:\n",
      "\n",
      " Pred    GT  Delta  Pct Correct\n",
      " 0.55  0.62   0.51         14.0\n",
      " 0.55  0.67   0.45         14.0\n",
      " 0.56  0.69   0.45         14.0\n",
      " 0.43  0.69   0.58          0.0\n",
      " 0.61  0.70   0.43         14.0\n",
      " 0.43  0.51   0.40         14.0\n",
      " 0.55  0.64   0.51          0.0\n",
      " 0.57  0.65   0.38         14.0\n",
      " 0.58  0.60   0.49          0.0\n",
      " 0.71  0.58   0.32          0.0\n",
      " 0.57  0.65   0.49          0.0\n",
      " 0.71  0.73   0.46         14.0\n",
      " 0.44  0.62   0.60          0.0\n",
      " 0.44  0.64   0.65          0.0\n",
      " 0.58  0.61   0.52          0.0\n",
      " 0.56  0.62   0.40          0.0\n",
      " 0.69  0.70   0.35         29.0\n",
      " 0.58  0.53   0.55          0.0\n",
      " 0.68  0.65   0.35          0.0\n",
      " 0.61  0.65   0.40         29.0\n",
      " 0.57  0.71   0.50         29.0\n",
      " 0.49  0.56   0.46          0.0\n",
      " 0.57  0.67   0.51         14.0\n",
      " 0.65  0.63   0.35         29.0\n",
      " 0.58  0.70   0.56         14.0\n",
      " 0.44  0.67   0.50          0.0\n",
      " 0.57  0.71   0.40         29.0\n",
      " 0.71  0.77   0.48         29.0\n",
      " 0.55  0.68   0.35         29.0\n",
      " 0.57  0.65   0.41          0.0\n",
      " 0.51  0.57   0.58          0.0\n",
      " 0.71  0.55   0.56          0.0\n",
      " 0.29  0.67   0.68          0.0\n",
      " 0.57  0.72   0.47         14.0\n",
      " 0.42  0.67   0.54          0.0\n",
      " 0.71  0.67   0.36         29.0\n",
      " 0.57  0.66   0.54         14.0\n",
      " 0.61  0.65   0.44         14.0\n",
      " 0.71  0.79   0.29         43.0\n",
      " 0.71  0.50   0.57          0.0\n",
      " 0.65  0.74   0.33         14.0\n",
      " 0.40  0.67   0.55         14.0\n",
      " 0.57  0.69   0.50          0.0\n",
      " 0.70  0.70   0.43         29.0\n",
      " 0.57  0.58   0.46          0.0\n",
      " 0.71  0.67   0.54         14.0\n",
      " 0.82  0.55   0.38         29.0\n",
      " 0.71  0.66   0.52          0.0\n",
      " 0.43  0.56   0.49          0.0\n",
      " 0.57  0.64   0.49         14.0\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.46353807, Accuracy all channels correct: 0/10000 (0%)\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training epoch 1\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.482821\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.434844\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.436339\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.431875\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.426094\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.402344\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.383036\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.391652\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.389286\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.380625\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.398125\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.374665\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.400112\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.386183\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.367835\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.388929\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.388281\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.405357\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.394308\n",
      "\n",
      "At end of training epoch 1, Avg. loss is 0.40054366\n",
      "\n",
      "\n",
      "Testing epoch 1\n",
      "\n",
      "\n",
      "Random sample of mean predictions across channels for test set, where each row is a test sample in the training batch:\n",
      "\n",
      " Pred    GT  Delta  Pct Correct\n",
      " 0.86  0.58   0.37         14.0\n",
      " 0.86  0.64   0.34         14.0\n",
      " 0.86  0.58   0.44         29.0\n",
      " 0.86  0.69   0.41          0.0\n",
      " 0.86  0.62   0.45          0.0\n",
      " 0.86  0.57   0.38         14.0\n",
      " 0.86  0.70   0.30         14.0\n",
      " 0.86  0.69   0.37         43.0\n",
      " 0.86  0.55   0.48          0.0\n",
      " 0.86  0.66   0.44         14.0\n",
      " 0.86  0.74   0.34         29.0\n",
      " 0.86  0.72   0.34         14.0\n",
      " 0.86  0.69   0.27         29.0\n",
      " 0.86  0.70   0.37         29.0\n",
      " 0.86  0.73   0.24         29.0\n",
      " 0.86  0.62   0.41         14.0\n",
      " 0.86  0.74   0.39         29.0\n",
      " 0.86  0.52   0.42          0.0\n",
      " 0.86  0.68   0.45         29.0\n",
      " 0.86  0.50   0.47         14.0\n",
      " 0.86  0.72   0.38         29.0\n",
      " 0.86  0.64   0.47         14.0\n",
      " 0.86  0.69   0.41         29.0\n",
      " 0.86  0.70   0.33         29.0\n",
      " 0.86  0.60   0.45          0.0\n",
      " 0.86  0.69   0.32         29.0\n",
      " 0.86  0.70   0.44         29.0\n",
      " 0.86  0.50   0.52          0.0\n",
      " 0.86  0.65   0.42         14.0\n",
      " 0.86  0.68   0.42          0.0\n",
      " 0.86  0.55   0.40          0.0\n",
      " 0.86  0.71   0.37         14.0\n",
      " 0.86  0.63   0.32         29.0\n",
      " 0.86  0.59   0.36         14.0\n",
      " 0.86  0.71   0.33         29.0\n",
      " 0.86  0.62   0.42         29.0\n",
      " 0.86  0.63   0.44         29.0\n",
      " 0.86  0.64   0.41         14.0\n",
      " 0.86  0.57   0.44         14.0\n",
      " 0.86  0.61   0.40         14.0\n",
      " 0.86  0.63   0.43         14.0\n",
      " 0.86  0.66   0.38         29.0\n",
      " 0.86  0.69   0.41          0.0\n",
      " 0.86  0.51   0.46          0.0\n",
      " 0.86  0.47   0.50          0.0\n",
      " 0.86  0.67   0.39         14.0\n",
      " 0.86  0.63   0.33         14.0\n",
      " 0.86  0.72   0.28         29.0\n",
      " 0.86  0.69   0.31         29.0\n",
      " 0.86  0.72   0.36         29.0\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.39244699, Accuracy all channels correct: 0/10000 (0%)\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training epoch 2\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.395067\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.404062\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.407455\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.391495\n"
     ]
    }
   ],
   "source": [
    "# To ensure reproducibility, ensure we never attempt to train without also reseting the random seed every time.\n",
    "if training_runs_count >= seed_reset_count:\n",
    "  msg = \"You didn't reset the random seed! Runs won't be reproducible. Re-run reseting random seed.\"\n",
    "  raise Exception(msg)\n",
    "\n",
    "training_runs_count += 1\n",
    "\n",
    "def time_wrapper():\n",
    "  test(epoch=0)\n",
    "  for epoch in range(1, n_epochs + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "\n",
    "%time time_wrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Best Results\n",
    "\n",
    "<b>04b_simple_toy1: Test set: Avg. loss: 0.0081, Accuracy: 4984/5000 (99%)</b><br/>\n",
    "<b>03b_simple_toy1: Test set: Avg. loss: 0.0440, Accuracy: 9872/10000 (98%)</b><br/>\n",
    "<b>02b_simple_toy1: Test set: Avg. loss: 0.0393, Accuracy: 9870/10000 (98%)</b><br/>\n",
    "<b>01b_simple_toy1: Test set: Avg. loss: 0.0569, Accuracy: 9818/10000 (98%)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(range(1, n_epochs + 1), train_losses, color='blue')\n",
    "# TODO: Something seems wrong to me, the test loss shouldn't be below the training loss.\n",
    "plt.scatter(range(1, n_epochs + 1), test_losses[1:], color='red')\n",
    "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('L1 norm loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Example With Trained Network From Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets, example_debug) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  output = model(example_data.to(device))\n",
    "  output = output.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "  plt.subplot(2,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  # TODO: Don't we have problems with imshow() changing the brightness?\n",
    "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Prediction: {}\".format(\n",
    "    output.data.max(1, keepdim=True)[1][i].item()))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue Training From Saved Checkpoint &amp; Final Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test whether all this old code still works with all the experiment\n",
    "# changes.\n",
    "\n",
    "# continued_network = Net()\n",
    "# continued_optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "#                                 momentum=momentum)\n",
    "\n",
    "# network_state_dict = torch.load(model_path)\n",
    "# continued_network.load_state_dict(network_state_dict)\n",
    "\n",
    "# optimizer_state_dict = torch.load(optimizer_path)\n",
    "# continued_optimizer.load_state_dict(optimizer_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def continue_training():\n",
    "#   for i in range(4, 9):\n",
    "#     test_counter.append(i*len(train_loader.dataset))\n",
    "#     train(i)\n",
    "#     test()\n",
    "\n",
    "# %time continue_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure()\n",
    "# plt.plot(train_counter, train_losses, color='blue')\n",
    "# plt.scatter(test_counter, test_losses, color='red')\n",
    "# plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "# plt.xlabel('number of training examples seen')\n",
    "# plt.ylabel('negative log likelihood loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
