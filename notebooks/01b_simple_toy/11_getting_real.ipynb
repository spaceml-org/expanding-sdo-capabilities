{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Standard Candle\" 11: Start Getting Real\n",
    "\n",
    "Combine the work in experiment 10 with the older work in experiment 6 to have a single solid start, then bring in the basic dataloader to start getting real SDO images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this notebook should be run from the `notebooks/` subdirectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Change this to what the notebook name is for each experiment to ensure\n",
    "# training results are saved into the right sub-directory.\n",
    "notebook_name = '11_getting_real'\n",
    "\n",
    "from collections import namedtuple, defaultdict\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import shutil\n",
    "import pdb\n",
    "from functools import reduce\n",
    "import operator\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data and training results.\n",
    "root_path = '../..' # Relative to: notebooks/01b_simple_toy\n",
    "results_path = os.path.join(root_path, 'training_results', notebook_name)\n",
    "model_path = os.path.join(results_path, 'model.pth')\n",
    "optimizer_path = os.path.join(results_path, 'optimizer.pth')\n",
    "\n",
    "for path in [results_path]:\n",
    "  if not os.path.exists(path):\n",
    "    print('{} does not exist; creating directory...'.format(os.path.abspath(path)))\n",
    "    os.makedirs(path)\n",
    "\n",
    "num_epochs = 5\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "log_interval = 10\n",
    "height = 28\n",
    "width = 28\n",
    "num_channels = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_gpu(cuda_device=0):\n",
    "  \"\"\" Use the GPU. \"\"\"\n",
    "  torch.backends.cudnn.enabled = True\n",
    "  if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"CUDA not available! Unable to continue\")\n",
    "  # Force ourselves to use only one GPU.\n",
    "  device = torch.device(\"cuda:{}\".format(cuda_device))\n",
    "  print(\"Using device {} for training, current device: {}, total devices: {}\".format(\n",
    "    device, torch.cuda.current_device(), torch.cuda.device_count()))\n",
    "  return device\n",
    "\n",
    "def set_seed(random_seed=1, deterministic_cuda=True):\n",
    "  \"\"\" Force runs to be deterministic and reproducible. \"\"\"\n",
    "  np.random.seed(random_seed)\n",
    "  random.seed(random_seed)\n",
    "  torch.manual_seed(random_seed)\n",
    "  torch.cuda.manual_seed(random_seed)\n",
    "  os.environ['PYTHONHASHSEED'] = str(random_seed)\n",
    "\n",
    "  # Note: this can have a performance hit.\n",
    "  if deterministic_cuda:\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = init_gpu(cuda_device=1)\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_shape, output_dim):\n",
    "        super().__init__()\n",
    "        if (len(input_shape) != 3):\n",
    "            raise ValueError('Expecting an input_shape representing dimensions CxHxW')\n",
    "        self._input_channels = input_shape[0]\n",
    "        print('input_channels: {}'.format(self._input_channels))\n",
    "        self._conv2d1 = nn.Conv2d(in_channels=self._input_channels, out_channels=64, kernel_size=3)\n",
    "        self._conv2d2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3)\n",
    "        self._cnn_output_dim = self._cnn(torch.zeros(input_shape).unsqueeze(0)).nelement()\n",
    "        print('cnn_output_dim: {}'.format(self._cnn_output_dim))\n",
    "        self._fc1 = nn.Linear(self._cnn_output_dim, 256)\n",
    "        self._fc2 = nn.Linear(256, output_dim)\n",
    "        \n",
    "    def _cnn(self, x):\n",
    "        x = self._conv2d1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = nn.MaxPool2d(kernel_size=3)(x)\n",
    "        x = self._conv2d2(x)\n",
    "        x = nn.MaxPool2d(kernel_size=3)(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_dim = x.shape[0]\n",
    "        x = self._cnn(x).view(batch_dim, -1)\n",
    "        x = self._fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self._fc2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticSDODataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, num_channels, height, width, size, length):\n",
    "    self._num_channels = num_channels\n",
    "    self._height = height\n",
    "    self._width = width\n",
    "    self._size = size\n",
    "    self._length = length\n",
    "    \n",
    "    # TODO: Compute mean and std across dataset, and normalize them.\n",
    "    \n",
    "  def __getitem__(self, idx):\n",
    "    sun = self._draw_sun(self._num_channels, self._size, self._height, self._width)\n",
    "    dimmed_sun = sun.clone().to(device)\n",
    "    dim_factor = torch.rand(num_channels).to(device)    \n",
    "    for c in range(num_channels):\n",
    "      dimmed_sun[c] *= dim_factor[c]\n",
    "      \n",
    "    return dimmed_sun, dim_factor, sun\n",
    "    \n",
    "  def __len__(self):\n",
    "    return self._length\n",
    "  \n",
    "  def _draw_sun(self, num_channels, size, height, width):\n",
    "    \"\"\" Draw a synthetic sun. \"\"\"\n",
    "    xx, yy = np.meshgrid(np.arange(size)-(size-1)/2.,\n",
    "                         np.arange(size)-(size-1)/2)\n",
    "    r = np.sqrt(xx*xx+yy*yy) \n",
    "    R = np.random.rand(1)*(size/3)\n",
    "    channels = []\n",
    "    for c in range(num_channels):\n",
    "        I_c = R**(c/10.)\n",
    "        channels.append(np.exp(-(r*r)/(R*R))*I_c)\n",
    "\n",
    "    image = [torch.from_numpy(channel).float().to(device) for channel in channels]\n",
    "    image = torch.cat(image).view(num_channels, height, width)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SyntheticSDODataset(num_channels, height, width, size=height, length=10000)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size_train,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = SyntheticSDODataset(num_channels, height, width, size=height, length=1000)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size_test,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample_image(loader):\n",
    "  print('\\nUndimmed channels for a single original sun:\\n')\n",
    "  _, _, orig_sun = loader.dataset[0]\n",
    "  sun_numpy = orig_sun.cpu().numpy()\n",
    "  for channel in sun_numpy:\n",
    "    plt.imshow(channel, norm=None, cmap='hot', vmin=sun_numpy.min(), vmax=sun_numpy.max())\n",
    "    plt.show()\n",
    "\n",
    "def print_details(orig_data, output, dimmed_data, dim_factors, train):\n",
    "  print('\\n\\nDetails with sample from final batch:')\n",
    "  data_min, data_max = torch.min(orig_data), torch.max(orig_data)\n",
    "  sample = orig_data[0].cpu().numpy()\n",
    "  sample_dimmed = dimmed_data[0].cpu().numpy()\n",
    "\n",
    "  for i, (channel, channel_dimmed) in enumerate(zip(sample, sample_dimmed)):\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(1, 3, 1)\n",
    "    ax1.imshow(channel, norm=None, cmap='hot', vmin=data_min, vmax=data_max)\n",
    "    ax2 = fig.add_subplot(1, 3, 2)\n",
    "    ax2.imshow(channel_dimmed, norm=None, cmap='hot', vmin=data_min, vmax=data_max)\n",
    "    ax3 = fig.add_subplot(1, 3, 3)\n",
    "    ax3.imshow(channel_dimmed / float(output[0, i]), norm=None, cmap='hot', vmin=data_min, vmax=data_max)\n",
    "    print('\\nChannel: {} (left: original, middle: dimmed, right: undimmed)\\nDimming (true): {}, dimming (predicted): {}'.format(\n",
    "      i, dim_factors[0, i], output[0, i]))\n",
    "    plt.show()\n",
    "  dim_factors_numpy = dim_factors[0].view(-1).cpu().numpy()\n",
    "  plt.plot(dim_factors_numpy, label='Dimming factors (true)')\n",
    "  output_numpy = output[0].detach().view(-1).cpu().numpy()\n",
    "  plt.plot(output_numpy, label='Dimming factors (predicted)')\n",
    "  title = 'training dimming factors' if train else 'testing dimming factors'\n",
    "  plt.title(title)\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "def generate_train_accuracy_stats(correct, output, targets, num_column_labels, num_subsample):\n",
    "  # TODO: For efficiency reasons, convert all of this to torch rather than numpy operations\n",
    "  # so that we can do all this work on the GPU.\n",
    "  preds = output.detach().cpu().numpy()\n",
    "  targets = targets.detach().cpu().numpy()\n",
    "\n",
    "  # If a channel brightness prediction is within this percentage of the ground truth then we\n",
    "  # consider that prediction correct.\n",
    "  pct_close = 0.15\n",
    "\n",
    "  # Means across each row that collapses each batch entries predictions,\n",
    "  # the ground truth brightness, and the delta btw ground truth and prediction.\n",
    "  mean_per_channel_prediction = preds.mean(axis=0)\n",
    "  mean_per_channel_gt = targets.mean(axis=0)\n",
    "  per_channel_diff = np.abs(targets - preds)\n",
    "  mean_per_channel_diff = per_channel_diff.mean(axis=0)\n",
    "\n",
    "  # Various stats around channel correctness.\n",
    "  per_channel_correct = per_channel_diff <= np.abs(pct_close * targets)\n",
    "  # TODO: We can probably get rid of both of these np.where() conversions to 1/0s and just use the boolean\n",
    "  # array itself for the sum.\n",
    "  percentage_channels_correct = np.sum(np.where(per_channel_correct, 1, 0), axis=1, keepdims=True,\n",
    "                                       dtype=np.int)\n",
    "  correct_per_channel = np.sum(np.where(per_channel_correct, 1, 0), axis=1, keepdims=True,\n",
    "                               dtype=np.int)\n",
    "  pct_correct_per_channel = correct_per_channel / num_channels\n",
    "\n",
    "  # Which batch results have _all_ of their channel predictions fully correct?\n",
    "  # TODO: We can probably get rid of the np.where() conversion to 1/0s and just use the boolean\n",
    "  # array itself for the sum.\n",
    "  num_fully_correct_all_channels = np.where(pct_correct_per_channel == 1.0, 1, 0).sum()\n",
    "  pct_fully_correct_all_channels = num_fully_correct_all_channels / batch_size_test\n",
    "  correct += num_fully_correct_all_channels\n",
    "  \n",
    "  pretty_results = np.zeros((min(batch_size_test, len(preds)), num_column_labels), dtype=np.float32)\n",
    "\n",
    "  # The mean channel prediction across each row of the batch results.\n",
    "  pretty_results[:, 0] = np.round(preds.mean(axis=1), decimals=2)\n",
    "\n",
    "  # The mean channel ground truth across each row of the batch results.\n",
    "  pretty_results[:, 1] = np.round(targets.mean(axis=1), decimals=2)\n",
    "\n",
    "  # The mean difference btw prediction and grouth truth across each row of the batch results.\n",
    "  pretty_results[:, 2] = np.round(np.abs(targets - preds).mean(axis=1), decimals=2)\n",
    "\n",
    "  # Percentage correct across all the channels for a given batch row?\n",
    "  pretty_results[:, 3] = np.round(pct_correct_per_channel * 100.0, decimals=0)[:, 0].astype(np.int)\n",
    "  \n",
    "  # Randomly sub-sample some of the results since 100s or 1000s are too much to display.\n",
    "  lookup_idxs = np.random.choice(len(pretty_results), size=(num_subsample,))\n",
    "  pretty_results = pretty_results[lookup_idxs]\n",
    "\n",
    "  return correct, pretty_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sample_image(train_loader)\n",
    "\n",
    "model = Net(input_shape=[num_channels, height, width], output_dim=num_channels)\n",
    "model.cuda(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "  \n",
    "def train(epoch):\n",
    "  print(\"\\n\\n===================================\\n\\n\")\n",
    "  print(\"\\n\\nTraining epoch {}\\n\".format(epoch))\n",
    "  model.train()\n",
    "  losses = []\n",
    "  for batch_idx, (dimmed_data, dim_factors, orig_data) in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(dimmed_data)\n",
    "    loss = nn.MSELoss()(output, dim_factors)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(float(loss))\n",
    "\n",
    "    if batch_idx % log_interval == 0:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        epoch, batch_idx * len(orig_data), len(train_loader.dataset),\n",
    "        100.0 * (batch_idx / len(train_loader)), float(loss)))\n",
    "      # TODO: Save the model.\n",
    "      # torch.save(model.state_dict(), model_path)\n",
    "      # torch.save(optimizer.state_dict(), optimizer_path)\n",
    "\n",
    "  # Print extra debug output on the final batch.\n",
    "  print_details(orig_data, output, dimmed_data, dim_factors, train=True)\n",
    "\n",
    "  plt.plot(losses, label='training loss')\n",
    "  plt.title('training loss')\n",
    "  plt.show()\n",
    "  \n",
    "  print('\\nAt end of train epoch {}, loss min: {}, max: {}, mean: {}'.format(epoch,\n",
    "    min(losses), max(losses), np.mean(losses)))\n",
    "  \n",
    "  return np.mean(losses)\n",
    "\n",
    "def test(epoch):\n",
    "  print(\"\\n\\nTesting epoch {}\".format(epoch))\n",
    "  with torch.no_grad():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    correct = 0\n",
    "    current_batch = 0\n",
    "    num_subsample = 3 # Per batch, how many results to subsample to print out for debugging.\n",
    "    column_labels = ['Pred', 'GT', 'Delta', 'Pct Correct']\n",
    "    pretty_results = np.zeros((int(math.ceil(len(test_loader.dataset) / batch_size_test) * num_subsample),\n",
    "                               len(column_labels)), dtype=np.float32)\n",
    "    for batch_idx, (dimmed_data, dim_factors, orig_data) in enumerate(test_loader):\n",
    "      current_batch += 1\n",
    "      dimmed_data = dimmed_data.to(device)\n",
    "      dim_factors = dim_factors.to(device)\n",
    "      output = model(dimmed_data)\n",
    "      output = output.to(device)\n",
    "      loss = nn.MSELoss()(output, dim_factors)\n",
    "      losses.append(float(loss))\n",
    "\n",
    "      correct, pretty_print_subset = generate_train_accuracy_stats(\n",
    "        correct, output, dim_factors, len(column_labels), num_subsample)\n",
    "      current_batch_idx = current_batch - 1\n",
    "      pretty_results[current_batch_idx*num_subsample:current_batch_idx*num_subsample+num_subsample] = pretty_print_subset\n",
    "\n",
    "    # Print extra debug output on the final batch.\n",
    "    print_details(orig_data, output, dimmed_data, dim_factors, train=False)\n",
    "    \n",
    "    print(\"\\n\\nRandom sample of mean predictions across channels for test set, \"\n",
    "          \"where each row is a test sample in the training batch:\\n\")\n",
    "    df = pandas.DataFrame(pretty_results, columns=column_labels)\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    print('\\n\\nEpoch {}, test set: avg. loss: {:.8f}, Accuracy all channels correct: {}/{} ({:.0f}%)'.format(\n",
    "          epoch, np.mean(losses), correct, len(test_loader.dataset),\n",
    "          100.0 * (correct / len(test_loader.dataset))))\n",
    "\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "  train_losses = train(epoch)\n",
    "  test_losses = test(epoch)\n",
    "  \n",
    "  plt.plot(test_losses, label='testing loss')\n",
    "  plt.title('testing loss')\n",
    "  plt.show()\n",
    "  \n",
    "print('\\n\\nFinal mean training loss after {} epochs: {}'.format(\n",
    "  num_epochs, train_losses.mean()))\n",
    "print('Final mean testing loss after {} epochs: {}'.format(\n",
    "  num_epochs, test_losses.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
