{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export GOOGLE_APPLICATION_CREDENTIALS=/home/bradneuberg/expanding-sdo-capabilities/config/space_weather_sdo.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'fdl-sdo-data'\n",
    "path = 'SDOMLnpz/2010/09/01'\n",
    "max_threads = 32\n",
    "\n",
    "client = storage.Client()\n",
    "bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "blobs = list(bucket.list_blobs(prefix=path))\n",
    "\n",
    "def download_path(bucket, path):\n",
    "    data = bucket.blob(path).download_as_string()\n",
    "\n",
    "def download_blob(blob):\n",
    "    data = blob.download_as_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = []\n",
    "active_thread_count = 0\n",
    "for blob in blobs:\n",
    "    thread = threading.Thread(target=download_blob, kwargs={\"blob\": blob})\n",
    "    threads.append(thread)\n",
    "    thread.start()\n",
    "    active_thread_count += 1\n",
    "    if active_thread_count == max_threads:\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "        print('Downloaded {} files'.format(active_thread_count))\n",
    "        active_thread_count = 0\n",
    "        threads = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this module we define a pytorch SDO dataset\n",
    "\"\"\"\n",
    "import logging\n",
    "from os import path\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from sdo.io import sdo_find, sdo_scale\n",
    "from sdo.pytorch_utilities import to_tensor\n",
    "from sdo.ds_utility import minmax_normalization\n",
    "\n",
    "\n",
    "_logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class SDO_Dataset(Dataset):\n",
    "    \"\"\" Custom Dataset class compatible with torch.utils.data.DataLoader.\n",
    "    It can be used to flexibly load a train or test dataset from the SDO local folder,\n",
    "    asking for a specific range of years and a specific frequency in months, days, hours,\n",
    "    minutes. Scaling is applied by default, normalization can be optionally applied. \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        gcp_bucket_name,\n",
    "        inventory_path,\n",
    "        instr=[\"AIA\", \"AIA\", \"HMI\"],\n",
    "        channels=[\"0171\", \"0193\", \"bz\"],\n",
    "        yr_range=[2010, 2018],\n",
    "        mnt_step=1,\n",
    "        day_step=1,\n",
    "        h_step=6,\n",
    "        min_step=60,\n",
    "        resolution=512,\n",
    "        subsample=1,\n",
    "        test=False,\n",
    "        test_ratio=0.3,\n",
    "        shuffle=False,\n",
    "        normalization=0,\n",
    "        scaling=True,\n",
    "        holdout=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_basedir (str): path to locate training/testing data.\n",
    "            data_inventory (str): path to a pre-computed inventory file that contains\n",
    "                a dataframe of existing files. If False(or not valid) the file search is done\n",
    "                by folder and it is much slower.\n",
    "            channels (list string): channels to be selected\n",
    "            instr (list string): instrument to which each channel corresponds to. \n",
    "                                 It has to be of the same size of channels.\n",
    "            yr_range (list int): range of years to be selected\n",
    "            mnt_step (int): month frequency\n",
    "            day_step (int): day frequency\n",
    "            h_step (int): hour frequency\n",
    "            min_step (int): minute frequency\n",
    "            resolution (int): original resolution\n",
    "            base_dir (str): path to the main dataset folder\n",
    "            test (bool): if True, a test dataset is returned. By default the training dataset is returned.\n",
    "            test_ratio (float): percentage of data to be used for testing. Training ratio is 1-test_ratio.\n",
    "            shuffle (bool): if True, the dataset will be shuffled. Keep it False if you want to return\n",
    "                            a time-ordered dataset.\n",
    "            subsample (int): if 1 resolution of the final images will be as the original. \n",
    "                             If > 1 the image is downsampled. i.e. if resolution=512 and \n",
    "                             subsample=4, the images will be 128*128\n",
    "            normalization (int): if 0 normalization is not applied, if > 0 a normalization\n",
    "                                 by image is applied (only one type of normalization implemented \n",
    "                                 for now)\n",
    "            scaling (bool): if True pixel values are scaled by the expected max value in active regions\n",
    "                            (see sdo.io.sdo_scale)\n",
    "            holdout (bool): if True use the holdout as test set. test_ratio is ignored in this case.\n",
    "        \"\"\"\n",
    "        assert day_step > 0 and h_step > 0 and min_step > 0\n",
    "\n",
    "        self.data_basedir = data_basedir\n",
    "        self.instr = instr\n",
    "        self.channels = channels\n",
    "        self.resolution = resolution\n",
    "        self.subsample = subsample\n",
    "        self.shuffle = shuffle\n",
    "        self.yr_range = yr_range\n",
    "        self.mnt_step = mnt_step\n",
    "        self.day_step = day_step\n",
    "        self.h_step = h_step\n",
    "        self.min_step = min_step\n",
    "        self.test = test\n",
    "        self.test_ratio = test_ratio\n",
    "        self.normalization = normalization\n",
    "        self.scaling = scaling\n",
    "        self.holdout = holdout\n",
    "        if path.isfile(data_inventory):\n",
    "            self.data_inventory = data_inventory\n",
    "        else:\n",
    "            _logger.warning(\"A valid inventory file has NOT be passed\"\n",
    "                            \"If this is not expected check the path.\")\n",
    "            self.data_inventory = False\n",
    "        # TODO self.timestamps is not used in get_item\n",
    "        self.files, self.timestamps = self.create_list_files()\n",
    "\n",
    "    def find_months(self):\n",
    "        \"select months for training and test based on test ratio\"\n",
    "        # November and December are kept as holdout\n",
    "        if not self.holdout:\n",
    "            months = np.arange(1, 11, self.mnt_step)\n",
    "            if self.test:\n",
    "                n_months = int(len(months) * self.test_ratio)\n",
    "                months = months[-n_months:]\n",
    "                _logger.info('Testing on months \"%s\"' % months)\n",
    "            else:\n",
    "                n_months = int(len(months) * (1 - self.test_ratio))\n",
    "                months = months[:n_months]\n",
    "                _logger.info('Training on months \"%s\"' % months)\n",
    "        else:\n",
    "            n_months = [11, 12]\n",
    "        return months\n",
    "\n",
    "    def create_list_files(self):\n",
    "        \"\"\"\n",
    "        Find path to files that correspond to the requested timestamps. A timestamp\n",
    "        is returned only if the files from ALL the requested channels are found.\n",
    "\n",
    "        Returns: list of lists of strings, list of tuples. The first argument are the \n",
    "             path to the files, each row is a timestamp. The second argument are the\n",
    "             correspondant timestamps.\n",
    "\n",
    "        \"\"\"\n",
    "        _logger.info('Loading SDOML from \"%s\"' % self.data_basedir)\n",
    "        _logger.info('Loading SDOML inventory file from \"%s\"' % self.data_inventory)\n",
    "        indexes = ['year', 'month', 'day', 'hour', 'min']\n",
    "        yrs = np.arange(self.yr_range[0], self.yr_range[1]+1)\n",
    "        months = self.find_months()\n",
    "        days = np.arange(1, 32, self.day_step)\n",
    "        hours = np.arange(0, 24, self.h_step)\n",
    "        minus = np.arange(0, 60, self.min_step)\n",
    "        tot_timestamps = np.prod([len(x) for x in [yrs, months, days, hours, minus]])\n",
    "        _logger.debug(\"Timestamps requested values: \")\n",
    "        _logger.debug(\"Years: %s\" % ','.join('{}'.format(i) for i in (yrs)))\n",
    "        _logger.debug(\"Months: %s\" % ','.join('{}'.format(i) for i in (months)))\n",
    "        _logger.debug(\"Days: %s\" % ','.join('{}'.format(i) for i in (days)))\n",
    "        _logger.debug(\"Hours: %s\" % ','.join('{}'.format(i) for i in (hours)))\n",
    "        _logger.debug(\"Minutes: %s\" % ','.join('{}'.format(i) for i in (minus)))\n",
    "        _logger.info(\"Max number of timestamps: %d\" % tot_timestamps)\n",
    "\n",
    "        if self.data_inventory:\n",
    "            df = pd.read_pickle(self.data_inventory)\n",
    "            cond0 = df['channel'].isin(self.channels)\n",
    "            cond1 = df['year'].isin(yrs)\n",
    "            cond2 = df['month'].isin(months)\n",
    "            cond3 = df['day'].isin(days)\n",
    "            cond4 = df['hour'].isin(hours)\n",
    "            cond5 = df['min'].isin(minus)\n",
    "\n",
    "            sel_df = df[cond0 & cond1 & cond2 & cond3 & cond4 & cond5]\n",
    "            n_sel_timestamps = sel_df.groupby(indexes).head(1).shape[0]\n",
    "            _logger.info(\"Timestamps found in the inventory: %d (%.2f)\" % \n",
    "                         (n_sel_timestamps, float(n_sel_timestamps)/tot_timestamps))\n",
    "            grouped_df = sel_df.groupby(indexes).size()\n",
    "            # we select only timestamp that have files for all the channels\n",
    "            grouped_df = grouped_df[grouped_df == len(self.channels)].to_frame()\n",
    "            sel_df = sel_df.reset_index().drop('index', axis=1)\n",
    "            sel_df = pd.merge(grouped_df, sel_df, how='inner',\n",
    "                              left_on=indexes, right_on=indexes)\n",
    "            # sorting is essential, the order of the channels must be consistent\n",
    "            s_files = sel_df.sort_values('channel').groupby(indexes)['file'].apply(list)\n",
    "            files = s_files.values.tolist()\n",
    "            timestamps = s_files.index.tolist()\n",
    "            discarded_tm = n_sel_timestamps - len(timestamps)\n",
    "        else:\n",
    "            _logger.warning(\n",
    "                'A valid inventory file has not been passed in, be prepared to wait.')\n",
    "            files = []\n",
    "            timestamps = []\n",
    "            discarded_tm = 0\n",
    "            for y in yrs:\n",
    "                for month in months:\n",
    "                    for d in days:\n",
    "                        for h in hours:\n",
    "                            for minu in minus:\n",
    "                                # if a single channel is missing for the combination\n",
    "                                # of parameters result is -1\n",
    "                                result = sdo_find(y, month, d, h, minu,\n",
    "                                                  initial_size=self.resolution,\n",
    "                                                  basedir=self.data_basedir,\n",
    "                                                  instrs=self.instr,\n",
    "                                                  channels=self.channels,\n",
    "                                                  )\n",
    "                            if result != -1:\n",
    "                                files.append(result)\n",
    "                                timestamp = (y, month, d, h, minu)\n",
    "                                timestamps.append(timestamp)\n",
    "                            else:\n",
    "                                discarded_tm += 1\n",
    "        if len(files) == 0:\n",
    "            _logger.error(\"No input images found\")\n",
    "        else:\n",
    "            _logger.info(\"N timestamps discarded because channel is missing = %d (%.5f)\" % \n",
    "                         (discarded_tm, float(discarded_tm)/n_sel_timestamps))\n",
    "            _logger.info(\"Selected timestamps = %d\" % len(files))\n",
    "            _logger.info(\"N images = %d\" % (len(files)*len(self.channels)))\n",
    "            if self.shuffle:\n",
    "                _logger.warning(\n",
    "                    \"Shuffling is being applied, this will alter the time sequence.\")\n",
    "                random.shuffle(files)\n",
    "        return files, timestamps\n",
    "\n",
    "    def normalize_by_img(self, img, norm_type):\n",
    "        if norm_type == 1:\n",
    "            return minmax_normalization(img)\n",
    "        else:\n",
    "            _logger.error(\"This type of normalization is not implemented.\"\n",
    "                          \"Original image is returned\")\n",
    "            return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        This function will return a single row of the dataset, where each image has \n",
    "        been scaled and normalized if requested in the class initialization.\n",
    "        Args:\n",
    "            index (int): dataset row index\n",
    "\n",
    "        Returns: pytorch tensor\n",
    "\n",
    "        \"\"\"\n",
    "        size = int(self.resolution / self.subsample)\n",
    "        n_channels = len(self.channels)\n",
    "        # the original images are NOT bytescaled\n",
    "        # we directly convert to 32 because the pytorch tensor will need to be 32\n",
    "        item = np.zeros(shape=(n_channels, size, size), dtype=np.float32)\n",
    "        for c in range(n_channels):\n",
    "            img = np.memmap(self.files[index][c], shape=(self.resolution, self.resolution), mode='r',\n",
    "                            dtype=np.float32)\n",
    "            if self.subsample > 1:\n",
    "                # Use numpy trick to essentially downsample the full resolution image by 'subsample'.\n",
    "                img = img[::self.subsample, ::self.subsample]\n",
    "            if self.scaling:\n",
    "                # divide by roughly the mean of the channel\n",
    "                img = sdo_scale(img, self.channels[c])\n",
    "            if self.normalization > 0:\n",
    "                img = self.normalize_by_img(img, self.normalization)\n",
    "        \n",
    "            item[c, :, :] = img\n",
    "   \n",
    "        # Note: For efficiency reasons, don't send each item to the GPU;\n",
    "        # rather, later, send the entire batch to the GPU.\n",
    "        return to_tensor(item, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcp_bucket_name = 'fdl-sdo-data'\n",
    "inventory_path = \n",
    "train_dataset = SDO_Dataset(data_basedir=data_basedir,\n",
    "                            data_inventory=data_inventory,\n",
    "                            num_channels=self.num_channels,\n",
    "                            instr=instruments,\n",
    "                            channels=wavelengths, yr_range=yr_range,\n",
    "                            mnt_step=mnt_step, day_step=day_step,\n",
    "                            h_step=h_step, min_step=min_step,\n",
    "                            resolution=actual_resolution,\n",
    "                            subsample=subsample,\n",
    "                            normalization=0, scaling=scaling,\n",
    "                            test_ratio=test_ratio,\n",
    "                            min_alpha=min_alpha,\n",
    "                            max_alpha=max_alpha,\n",
    "                            scaled_height=scaled_height,\n",
    "                            scaled_width=scaled_width,\n",
    "                            noise_image=noise_image,\n",
    "                            threshold_black=threshold_black,\n",
    "                            threshold_black_value=threshold_black_value,\n",
    "                            shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
