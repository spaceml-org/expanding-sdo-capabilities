{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from google.cloud import storage\n",
    "from google.cloud.storage.blob import Blob\n",
    "\n",
    "\n",
    "client = storage.Client()\n",
    "bucket = client.get_bucket('fdl-sdo-data')\n",
    "\n",
    "inventory_data = bucket.blob('SDOMLnpz/inventory.pkl').download_as_string()\n",
    "df = pd.read_pickle(BytesIO(inventory_data), compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'channel': '0335', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0000_0335.npz'}, {'channel': '0304', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0000_0304.npz'}, {'channel': '1600', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0000_1600.npz'}, {'channel': '0211', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0000_0211.npz'}, {'channel': '0171', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0000_0171.npz'}, {'channel': '0131', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0000_0131.npz'}, {'channel': '0094', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0000_0094.npz'}, {'channel': '0193', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0000_0193.npz'}]\n",
      "[{'channel': '1600', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0006_1600.npz'}, {'channel': '0335', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0006_0335.npz'}, {'channel': '1700', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0006_1700.npz'}, {'channel': '0304', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0006_0304.npz'}, {'channel': '0193', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0006_0193.npz'}, {'channel': '0171', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0006_0171.npz'}, {'channel': '0131', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0006_0131.npz'}, {'channel': '0094', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0006_0094.npz'}, {'channel': '0211', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0006_0211.npz'}]\n",
      "[{'channel': '1700', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0012_1700.npz'}, {'channel': '1600', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0012_1600.npz'}, {'channel': '0335', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0012_0335.npz'}, {'channel': '0304', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0012_0304.npz'}, {'channel': '0211', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0012_0211.npz'}, {'channel': '0193', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0012_0193.npz'}, {'channel': '0171', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0012_0171.npz'}, {'channel': '0131', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0012_0131.npz'}, {'channel': '0094', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0012_0094.npz'}]\n",
      "[{'channel': '1700', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0018_1700.npz'}, {'channel': '1600', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0018_1600.npz'}, {'channel': '0335', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0018_0335.npz'}, {'channel': '0304', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0018_0304.npz'}, {'channel': '0193', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0018_0193.npz'}, {'channel': '0171', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0018_0171.npz'}, {'channel': '0131', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0018_0131.npz'}, {'channel': '0094', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0018_0094.npz'}, {'channel': '0211', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0018_0211.npz'}]\n",
      "[{'channel': '1700', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0024_1700.npz'}, {'channel': '1600', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0024_1600.npz'}, {'channel': '0335', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0024_0335.npz'}, {'channel': '0304', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0024_0304.npz'}, {'channel': '0193', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0024_0193.npz'}, {'channel': '0171', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0024_0171.npz'}, {'channel': '0131', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0024_0131.npz'}, {'channel': '0094', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0024_0094.npz'}, {'channel': '0211', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0024_0211.npz'}]\n",
      "[{'channel': '1700', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0030_1700.npz'}, {'channel': '1600', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0030_1600.npz'}, {'channel': '0335', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0030_0335.npz'}, {'channel': '0304', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0030_0304.npz'}, {'channel': '0211', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0030_0211.npz'}, {'channel': '0171', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0030_0171.npz'}, {'channel': '0131', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0030_0131.npz'}, {'channel': '0094', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0030_0094.npz'}, {'channel': '0193', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0030_0193.npz'}]\n",
      "[{'channel': '0335', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0036_0335.npz'}, {'channel': '1700', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0036_1700.npz'}, {'channel': '1600', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0036_1600.npz'}, {'channel': '0304', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0036_0304.npz'}, {'channel': '0193', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0036_0193.npz'}, {'channel': '0094', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0036_0094.npz'}, {'channel': '0211', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0036_0211.npz'}, {'channel': '0171', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0036_0171.npz'}, {'channel': '0131', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0036_0131.npz'}]\n",
      "[{'channel': '1700', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0042_1700.npz'}, {'channel': '1600', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0042_1600.npz'}, {'channel': '0335', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0042_0335.npz'}, {'channel': '0304', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0042_0304.npz'}, {'channel': '0193', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0042_0193.npz'}, {'channel': '0171', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0042_0171.npz'}, {'channel': '0131', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0042_0131.npz'}, {'channel': '0094', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0042_0094.npz'}, {'channel': '0211', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0042_0211.npz'}]\n",
      "[{'channel': '1700', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0048_1700.npz'}, {'channel': '1600', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0048_1600.npz'}, {'channel': '0335', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0048_0335.npz'}, {'channel': '0304', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0048_0304.npz'}, {'channel': '0211', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0048_0211.npz'}, {'channel': '0171', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0048_0171.npz'}, {'channel': '0094', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0048_0094.npz'}, {'channel': '0193', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0048_0193.npz'}, {'channel': '0131', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0048_0131.npz'}]\n",
      "[{'channel': '0304', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0054_0304.npz'}, {'channel': '1700', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0054_1700.npz'}, {'channel': '1600', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0054_1600.npz'}, {'channel': '0335', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0054_0335.npz'}, {'channel': '0211', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0054_0211.npz'}, {'channel': '0171', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0054_0171.npz'}, {'channel': '0131', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0054_0131.npz'}, {'channel': '0094', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0054_0094.npz'}, {'channel': '0193', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0054_0193.npz'}]\n",
      "[{'channel': '1600', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0100_1600.npz'}, {'channel': '0335', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0100_0335.npz'}, {'channel': '0304', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0100_0304.npz'}, {'channel': '0211', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0100_0211.npz'}, {'channel': '0193', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0100_0193.npz'}, {'channel': '0171', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0100_0171.npz'}, {'channel': '0131', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0100_0131.npz'}, {'channel': '0094', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0100_0094.npz'}]\n",
      "[{'channel': '1700', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0106_1700.npz'}, {'channel': '1600', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0106_1600.npz'}, {'channel': '0335', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0106_0335.npz'}, {'channel': '0304', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0106_0304.npz'}, {'channel': '0094', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0106_0094.npz'}, {'channel': '0193', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0106_0193.npz'}, {'channel': '0171', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0106_0171.npz'}, {'channel': '0131', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0106_0131.npz'}, {'channel': '0211', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0106_0211.npz'}]\n",
      "[{'channel': '1600', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0112_1600.npz'}, {'channel': 'bz', 'file': 'SDOMLnpz/2013/01/01/HMI20130101_0112_bz.npz'}, {'channel': '1700', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0112_1700.npz'}, {'channel': '0335', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0112_0335.npz'}, {'channel': 'bx', 'file': 'SDOMLnpz/2013/01/01/HMI20130101_0112_bx.npz'}, {'channel': '0211', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0112_0211.npz'}, {'channel': '0094', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0112_0094.npz'}, {'channel': 'by', 'file': 'SDOMLnpz/2013/01/01/HMI20130101_0112_by.npz'}, {'channel': '0304', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0112_0304.npz'}, {'channel': '0193', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0112_0193.npz'}, {'channel': '0171', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0112_0171.npz'}, {'channel': '0131', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0112_0131.npz'}]\n",
      "[{'channel': '1700', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0118_1700.npz'}, {'channel': '1600', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0118_1600.npz'}, {'channel': '0094', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0118_0094.npz'}, {'channel': '0304', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0118_0304.npz'}, {'channel': '0211', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0118_0211.npz'}, {'channel': '0193', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0118_0193.npz'}, {'channel': '0171', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0118_0171.npz'}, {'channel': '0131', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0118_0131.npz'}, {'channel': '0335', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0118_0335.npz'}]\n",
      "[{'channel': 'bz', 'file': 'SDOMLnpz/2013/01/01/HMI20130101_0124_bz.npz'}, {'channel': 'by', 'file': 'SDOMLnpz/2013/01/01/HMI20130101_0124_by.npz'}, {'channel': 'bx', 'file': 'SDOMLnpz/2013/01/01/HMI20130101_0124_bx.npz'}, {'channel': '1700', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0124_1700.npz'}, {'channel': '0335', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0124_0335.npz'}, {'channel': '0304', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0124_0304.npz'}, {'channel': '0211', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0124_0211.npz'}, {'channel': '0193', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0124_0193.npz'}, {'channel': '0171', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0124_0171.npz'}, {'channel': '0131', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0124_0131.npz'}, {'channel': '0094', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0124_0094.npz'}, {'channel': '1600', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0124_1600.npz'}]\n",
      "[{'channel': '1700', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0130_1700.npz'}, {'channel': '0211', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0130_0211.npz'}, {'channel': '0193', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0130_0193.npz'}, {'channel': '0335', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0130_0335.npz'}, {'channel': '1600', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0130_1600.npz'}, {'channel': '0171', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0130_0171.npz'}, {'channel': '0131', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0130_0131.npz'}, {'channel': '0094', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0130_0094.npz'}, {'channel': '0304', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0130_0304.npz'}]\n",
      "[{'channel': '0171', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0136_0171.npz'}, {'channel': '1700', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0136_1700.npz'}, {'channel': '0335', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0136_0335.npz'}, {'channel': '0304', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0136_0304.npz'}, {'channel': '0211', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0136_0211.npz'}, {'channel': '0193', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0136_0193.npz'}, {'channel': '0131', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0136_0131.npz'}, {'channel': '0094', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0136_0094.npz'}, {'channel': '1600', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0136_1600.npz'}]\n",
      "[{'channel': '0193', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0142_0193.npz'}, {'channel': '1600', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0142_1600.npz'}, {'channel': '0335', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0142_0335.npz'}, {'channel': '0304', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0142_0304.npz'}, {'channel': '0211', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0142_0211.npz'}, {'channel': '0171', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0142_0171.npz'}, {'channel': '0131', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0142_0131.npz'}, {'channel': '0094', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0142_0094.npz'}, {'channel': '1700', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0142_1700.npz'}]\n",
      "[{'channel': '1700', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0148_1700.npz'}, {'channel': '0335', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0148_0335.npz'}, {'channel': '0304', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0148_0304.npz'}, {'channel': '0211', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0148_0211.npz'}, {'channel': '0193', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0148_0193.npz'}, {'channel': '0171', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0148_0171.npz'}, {'channel': '0131', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0148_0131.npz'}, {'channel': '0094', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0148_0094.npz'}, {'channel': '1600', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0148_1600.npz'}]\n",
      "[{'channel': '1600', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0154_1600.npz'}, {'channel': '1700', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0154_1700.npz'}, {'channel': '0094', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0154_0094.npz'}, {'channel': '0131', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0154_0131.npz'}, {'channel': '0171', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0154_0171.npz'}, {'channel': '0193', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0154_0193.npz'}, {'channel': '0211', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0154_0211.npz'}, {'channel': '0304', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0154_0304.npz'}, {'channel': '0335', 'file': 'SDOMLnpz/2013/01/01/AIA20130101_0154_0335.npz'}]\n"
     ]
    }
   ],
   "source": [
    "# \"No blob for SDOMLnpz/2013/07/31/AIA20130731_0000_all.pklz\"\n",
    "indexes = ['year', 'month', 'day', 'hour', 'min']\n",
    "counter = 0\n",
    "max_iter = 20\n",
    "year = 2013\n",
    "for _, timestamp_group in df[df['year'] == year].groupby(indexes):\n",
    "    channels = [\n",
    "        {'channel': row['channel'], 'file': row['file']}\n",
    "        for _, row in timestamp_group.iterrows() if '.pklz' not in row['file']\n",
    "    ]\n",
    "    print(channels)\n",
    "    \n",
    "    counter += 1\n",
    "    if max_iter is not None and counter >= max_iter:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sdo.pytorch_utilities import pass_seed_to_worker\n",
    "\n",
    "\n",
    "def create_dataloader(dataset, num_dataloader_workers, train):\n",
    "    # TODO: Move getting the number of dataloader workers over to parse_args.\n",
    "    assert num_dataloader_workers <= (multiprocessing.cpu_count() - 1), \\\n",
    "        'There are not enough CPU cores ({}) for requested dataloader ' \\\n",
    "        'workers ({})'.format(num_dataloader_workers, (multiprocessing.cpu_count() - 1))\n",
    "\n",
    "    _logger.info('Using {} workers for the {} pytorch DataLoader'.format(\n",
    "        num_dataloader_workers, 'training' if train else 'testing'))\n",
    "    loader = DataLoader(dataset,\n",
    "                        # We already shuffle things in the SDO_Dataset itself.\n",
    "                        shuffle=False,\n",
    "                        # Batches are generated in parallel in the SDO_Dataset itself.\n",
    "                        batch_size=1,\n",
    "                        num_workers=num_dataloader_workers,\n",
    "                        # Ensure workers spawn with the right newly\n",
    "                        # incremented random seed.\n",
    "                        worker_init_fn=pass_seed_to_worker,\n",
    "                        # Make sure that results returned from our\n",
    "                        # SDO_DataSet are placed onto the GPU.\n",
    "                        pin_memory=True)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this module we define a PyTorch SDO dataset.\n",
    "\"\"\"\n",
    "import bz2\n",
    "from math import ceil\n",
    "import logging\n",
    "from io import BytesIO\n",
    "from os import path\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "from threading import Thread\n",
    "\n",
    "from google.cloud import storage\n",
    "from google.cloud.storage.blob import Blob\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from sdo.io import sdo_find, sdo_scale\n",
    "from sdo.pytorch_utilities import to_tensor\n",
    "from sdo.ds_utility import minmax_normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# TODO!!! Have this also still work with NAS file paths.\n",
    "# TODO!!! Benchmark how well this works with the new combined timestamp files.\n",
    "class SDO_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset class compatible with torch.utils.data.DataLoader.\n",
    "    It can be used to flexibly load a train or test dataset from the SDO local folder,\n",
    "    asking for a specific range of years and a specific frequency in months, days, hours,\n",
    "    minutes. Scaling is applied by default, normalization can be optionally applied.\n",
    "\n",
    "    Note that the GCP storage bucket portion depends on the scripts combine_channels.py\n",
    "    and create_inventory.py having been run before to pre-optimize loading all the channels\n",
    "    per timestep.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        gcp_project,\n",
    "        gcp_bucket_name,\n",
    "        inventory_path,\n",
    "        batch_size=1,\n",
    "        instr=[\"AIA\", \"AIA\", \"HMI\"],\n",
    "        channels=[\"0171\", \"0193\", \"bz\"],\n",
    "        yr_range=[2010, 2018],\n",
    "        mnt_step=1,\n",
    "        day_step=1,\n",
    "        h_step=6,\n",
    "        min_step=60,\n",
    "        resolution=512,\n",
    "        subsample=1,\n",
    "        test=False,\n",
    "        test_ratio=0.3,\n",
    "        shuffle=False,\n",
    "        normalization=0,\n",
    "        scaling=True,\n",
    "        holdout=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            gcp_project (str): Google Cloud Provider project name, such as 'space-weather-sdo'.\n",
    "            gcp_bucket_name (str): Google Cloud Provider bucket name where data files are located,\n",
    "                such as 'fdl-sdo-data'.\n",
    "            inventory_path (str|bool): path on GCP to a pre-computed inventory file that contains\n",
    "                a dataframe of existing files. If False (or not valid) the file search is done\n",
    "                by folder and it is much slower.\n",
    "            batch_size (int): Batch size of results returned from __getitem__; we download the data\n",
    "                in parallel inside __getitem__ for efficiency reasons.\n",
    "            channels (list string): channels to be selected\n",
    "            instr (list string): instrument to which each channel corresponds to. \n",
    "                                 It has to be of the same size of channels.\n",
    "            yr_range (list int): range of years to be selected\n",
    "            mnt_step (int): month frequency\n",
    "            day_step (int): day frequency\n",
    "            h_step (int): hour frequency\n",
    "            min_step (int): minute frequency\n",
    "            resolution (int): original resolution\n",
    "            base_dir (str): path to the main dataset folder\n",
    "            test (bool): if True, a test dataset is returned. By default the training dataset is returned.\n",
    "            test_ratio (float): percentage of data to be used for testing. Training ratio is 1-test_ratio.\n",
    "            shuffle (bool): if True, the dataset will be shuffled. Keep it False if you want to return\n",
    "                            a time-ordered dataset.\n",
    "            subsample (int): if 1 resolution of the final images will be as the original. \n",
    "                             If > 1 the image is downsampled. i.e. if resolution=512 and \n",
    "                             subsample=4, the images will be 128*128\n",
    "            normalization (int): if 0 normalization is not applied, if > 0 a normalization\n",
    "                                 by image is applied (only one type of normalization implemented \n",
    "                                 for now)\n",
    "            scaling (bool): if True pixel values are scaled by the expected max value in active regions\n",
    "                            (see sdo.io.sdo_scale)\n",
    "            holdout (bool): if True use the holdout as test set. test_ratio is ignored in this case.\n",
    "        \"\"\"\n",
    "        assert day_step > 0 and h_step > 0 and min_step > 0\n",
    "\n",
    "        self.gcp_project = gcp_project\n",
    "        self.gcp_bucket_name = gcp_bucket_name\n",
    "        self.batch_size = batch_size\n",
    "        self.instr = instr\n",
    "        self.channels = channels\n",
    "        self.resolution = resolution\n",
    "        self.subsample = subsample\n",
    "        self.shuffle = shuffle\n",
    "        self.yr_range = yr_range\n",
    "        self.mnt_step = mnt_step\n",
    "        self.day_step = day_step\n",
    "        self.h_step = h_step\n",
    "        self.min_step = min_step\n",
    "        self.test = test\n",
    "        self.test_ratio = test_ratio\n",
    "        self.normalization = normalization\n",
    "        self.scaling = scaling\n",
    "        self.holdout = holdout\n",
    "        if inventory_path:\n",
    "            self.inventory_path = inventory_path\n",
    "        else:\n",
    "            _logger.warning(\"A valid inventory file has NOT be passed\"\n",
    "                            \"If this is not expected check the path.\")\n",
    "            self.inventory_path = False\n",
    "\n",
    "        # TODO self.timestamps is not used in get_item\n",
    "        self.files, self.timestamps = self.create_list_files()\n",
    "\n",
    "    def find_months(self):\n",
    "        \"select months for training and test based on test ratio\"\n",
    "        # November and December are kept as holdout\n",
    "        if not self.holdout:\n",
    "            months = np.arange(1, 11, self.mnt_step)\n",
    "            if self.test:\n",
    "                n_months = int(len(months) * self.test_ratio)\n",
    "                months = months[-n_months:]\n",
    "                _logger.info('Testing on months \"%s\"' % months)\n",
    "            else:\n",
    "                n_months = int(len(months) * (1 - self.test_ratio))\n",
    "                months = months[:n_months]\n",
    "                _logger.info('Training on months \"%s\"' % months)\n",
    "        else:\n",
    "            n_months = [11, 12]\n",
    "        return months\n",
    "\n",
    "    def create_list_files(self):\n",
    "        \"\"\"\n",
    "        Find path to files that correspond to the requested timestamps. A timestamp\n",
    "        is returned only if the files from ALL the requested channels are found.\n",
    "\n",
    "        Returns: list of lists of strings, list of tuples. The first argument are the \n",
    "             path to the files, each row is a timestamp. The second argument are the\n",
    "             correspondant timestamps.\n",
    "\n",
    "        \"\"\"\n",
    "        _logger.info('Loading SDOML from GCP bucket \"%s\"' % self.gcp_bucket_name)\n",
    "        _logger.info('Loading SDOML inventory file from \"%s\"' % self.inventory_path)\n",
    "        client, bucket = connect_gcp(self.gcp_project, self.gcp_bucket_name)\n",
    "        indexes = ['year', 'month', 'day', 'hour', 'min']\n",
    "        yrs = np.arange(self.yr_range[0], self.yr_range[1]+1)\n",
    "        months = self.find_months()\n",
    "        days = np.arange(1, 32, self.day_step)\n",
    "        hours = np.arange(0, 24, self.h_step)\n",
    "        minus = np.arange(0, 60, self.min_step)\n",
    "        tot_timestamps = np.prod([len(x) for x in [yrs, months, days, hours, minus]])\n",
    "        _logger.debug(\"Timestamps requested values: \")\n",
    "        _logger.debug(\"Years: %s\" % ','.join('{}'.format(i) for i in (yrs)))\n",
    "        _logger.debug(\"Months: %s\" % ','.join('{}'.format(i) for i in (months)))\n",
    "        _logger.debug(\"Days: %s\" % ','.join('{}'.format(i) for i in (days)))\n",
    "        _logger.debug(\"Hours: %s\" % ','.join('{}'.format(i) for i in (hours)))\n",
    "        _logger.debug(\"Minutes: %s\" % ','.join('{}'.format(i) for i in (minus)))\n",
    "        _logger.info(\"Max number of timestamps: %d\" % tot_timestamps)\n",
    "\n",
    "        if self.inventory_path:\n",
    "            inventory_data = bucket.blob(self.inventory_path).download_as_string()\n",
    "            df = pd.read_pickle(BytesIO(inventory_data), compression='gzip')\n",
    "            cond0 = df['channel'].isin(self.channels)\n",
    "            cond1 = df['year'].isin(yrs)\n",
    "            cond2 = df['month'].isin(months)\n",
    "            cond3 = df['day'].isin(days)\n",
    "            cond4 = df['hour'].isin(hours)\n",
    "            cond5 = df['min'].isin(minus)\n",
    "\n",
    "            sel_df = df[cond0 & cond1 & cond2 & cond3 & cond4 & cond5]\n",
    "            n_sel_timestamps = sel_df.groupby(indexes).head(1).shape[0]\n",
    "            _logger.info(\"Timestamps found in the inventory: %d (%.2f)\" % \n",
    "                         (n_sel_timestamps, float(n_sel_timestamps)/tot_timestamps))\n",
    "            grouped_df = sel_df.groupby(indexes).size()\n",
    "            # we select only timestamp that have files for all the channels\n",
    "            grouped_df = grouped_df[grouped_df == len(self.channels)].to_frame()\n",
    "            sel_df = sel_df.reset_index().drop('index', axis=1)\n",
    "            sel_df = pd.merge(grouped_df, sel_df, how='inner',\n",
    "                              left_on=indexes, right_on=indexes)\n",
    "            # sorting is essential, the order of the channels must be consistent\n",
    "            s_files = sel_df.sort_values('channel').groupby(indexes)['file'].apply(list)\n",
    "            files = s_files.values.tolist()\n",
    "            timestamps = s_files.index.tolist()\n",
    "            discarded_tm = n_sel_timestamps - len(timestamps)\n",
    "        else:\n",
    "            _logger.warning(\n",
    "                'A valid inventory file has not been passed in, be prepared to wait.')\n",
    "            files = []\n",
    "            timestamps = []\n",
    "            discarded_tm = 0\n",
    "            for y in yrs:\n",
    "                for month in months:\n",
    "                    for d in days:\n",
    "                        for h in hours:\n",
    "                            for minu in minus:\n",
    "                                # if a single channel is missing for the combination\n",
    "                                # of parameters result is -1\n",
    "                                # TODO: Get this working against listing the files in GCP buckets\n",
    "                                # directories.\n",
    "                                result = sdo_find(y, month, d, h, minu,\n",
    "                                                  initial_size=self.resolution,\n",
    "                                                  basedir=self.data_basedir,\n",
    "                                                  instrs=self.instr,\n",
    "                                                  channels=self.channels,\n",
    "                                                  )\n",
    "                            if result != -1:\n",
    "                                files.append(result)\n",
    "                                timestamp = (y, month, d, h, minu)\n",
    "                                timestamps.append(timestamp)\n",
    "                            else:\n",
    "                                discarded_tm += 1\n",
    "        if len(files) == 0:\n",
    "            _logger.error(\"No input images found\")\n",
    "        else:\n",
    "            _logger.info(\"N timestamps discarded because channel is missing = %d (%.5f)\" % \n",
    "                         (discarded_tm, float(discarded_tm)/n_sel_timestamps))\n",
    "            _logger.info(\"Selected timestamps = %d\" % len(files))\n",
    "            _logger.info(\"N images = %d\" % (len(files)*len(self.channels)))\n",
    "            if self.shuffle:\n",
    "                _logger.warning(\n",
    "                    \"Shuffling is being applied, this will alter the time sequence.\")\n",
    "                random.shuffle(files)\n",
    "        return files, timestamps\n",
    "\n",
    "    def __len__(self):\n",
    "        return ceil(len(self.files) / float(self.batch_size))\n",
    "\n",
    "    def __getitem__(self, batch_index):\n",
    "        \"\"\"\n",
    "        Download all of the items in our batch in parallel.\n",
    "        \"\"\"\n",
    "        start_idx = batch_index * self.batch_size\n",
    "        last_batch = batch_index == (len(self) - 1)\n",
    "        if last_batch:\n",
    "            end_idx = len(self.files) % self.batch_size\n",
    "        else:\n",
    "            end_idx = start_idx + (self.batch_size - 1)\n",
    "\n",
    "        #print('batch_size: {}, start_idx: {}, end_idx: {}, last_batch: {}'.format(self.batch_size, start_idx,\n",
    "        #                                                                          end_idx, last_batch))\n",
    "        item_paths = self.files[start_idx:end_idx+1]\n",
    "\n",
    "        # TODO!!! Have these threads stay around and each keep their own GCP connection long-lived.\n",
    "        threads = [None] * self.batch_size\n",
    "        results = [None] * self.batch_size\n",
    "        for i in range(self.batch_size):\n",
    "            threads[i] = Thread(target=download_item,\n",
    "                                args=(results, i, item_paths[i],\n",
    "                                      self.gcp_project, self.gcp_bucket_name,\n",
    "                                      self.resolution, self.subsample,\n",
    "                                      self.channels, self.scaling,\n",
    "                                      self.normalization))\n",
    "            threads[i].start()\n",
    "\n",
    "        # Wait for all the threads to finish downloading and transforming their\n",
    "        # results.\n",
    "        for i in range(self.batch_size):\n",
    "            threads[i].join()\n",
    "\n",
    "        # Note: For efficiency reasons, don't send each item to the GPU;\n",
    "        # rather, later, send the entire batch to the GPU.\n",
    "        return to_tensor(results, dtype=torch.float)\n",
    "\n",
    "\n",
    "# This runs in a thread, so we place it outside of the class to make it clear\n",
    "# what data it can interact with.\n",
    "def download_item(item_results, item_idx, item_paths, gcp_project, gcp_bucket_name, resolution,\n",
    "                  subsample, channels, scaling, normalization):\n",
    "    \"\"\"\n",
    "    This function will return a single row of the dataset, where each image has \n",
    "    been scaled and normalized if requested in the class initialization.\n",
    "    Args:\n",
    "        item_results (List[numpy]): List of numpy images with the results to make it\n",
    "                                    easy to return results to the parent thread.\n",
    "        item_idx (int): Where to place the results for this thread inside item_results.\n",
    "        item_paths (str[]): GCP paths for each of the channels for this item.\n",
    "        gcp_project (str): GCP project containing the GCP bucket.\n",
    "        gcp_bucket_name (str): Path to the GCP bucket to work with.\n",
    "        resolution (int): original resolution.\n",
    "        subsample (int): if 1 resolution of the final images will be as the original. \n",
    "                         If > 1 the image is downsampled. i.e. if resolution=512 and \n",
    "                         subsample=4, the images will be 128*128\n",
    "        channels (list string): channels to be selected\n",
    "        scaling (bool): if True pixel values are scaled by the expected max value in active regions\n",
    "                        (see sdo.io.sdo_scale)\n",
    "        normalization (int): if 0 normalization is not applied, if > 0 a normalization\n",
    "                             by image is applied (only one type of normalization implemented \n",
    "                             for now)\n",
    "\n",
    "    Returns: numpy array.\n",
    "    \"\"\"\n",
    "    wait_time_s = 1\n",
    "    # Continue forever until we get our data.\n",
    "    # UNCOMMENT!!!\n",
    "    #while True:\n",
    "    filename = get_combined_path(item_paths[0])\n",
    "    try:\n",
    "        client, bucket = connect_gcp(gcp_project, gcp_bucket_name)        \n",
    "        blob = bucket.get_blob(filename) # REVISIT!!!\n",
    "        if not blob:\n",
    "            # TODO!!! Make this cleaner.\n",
    "            print('No blob for {}'.format(filename))\n",
    "            return\n",
    "        # TODO!!! This is failing, figure out why.\n",
    "        all_channels = pickle.loads(bz2.decompress(blob.download_as_string()))\n",
    "        print(\"all_channels: {}\".format(all_channels))\n",
    "\n",
    "        n_channels = len(channels)\n",
    "        size = int(resolution / subsample)\n",
    "        # the original images are NOT bytescaled\n",
    "        # we directly convert to 32 because the pytorch tensor will need to be 32\n",
    "        item = np.zeros(shape=(n_channels, size, size), dtype=np.float32)\n",
    "\n",
    "        for c, channel_name in enumerate(channels):\n",
    "            assert channel_name in all_channels, '{} not in the combined timestamps!'.format(channel_name)\n",
    "            img_data = all_channels[channel_name]\n",
    "            print('channel_name: {}, value: {}'.format(channel_name, img_data)) # REMOVE!!!\n",
    "            img = np.load(BytesIO(img_data), allow_pickle=True)['x']\n",
    "            if subsample > 1:\n",
    "                # Use numpy trick to essentially downsample the full resolution image by 'subsample'.\n",
    "                img = img[::subsample, ::subsample]\n",
    "            if scaling:\n",
    "                # divide by roughly the mean of the channel\n",
    "                img = sdo_scale(img, channel)\n",
    "            if normalization > 0:\n",
    "                if normalization == 1:\n",
    "                    img = minmax_normalization(img)\n",
    "                else:\n",
    "                    _logger.error(\"This type of normalization is not implemented.\"\n",
    "                                  \"Original image is returned\")\n",
    "            item[c] = img\n",
    "\n",
    "        item_results[item_idx] = item\n",
    "        return\n",
    "    except Exception as e:\n",
    "        _logger.error(\"Exception for {}: {}\".format(filename, e))\n",
    "        # UNCOMMENT!!!\n",
    "#         time.sleep(wait_time_s * random.random())\n",
    "#         wait_time_s *= 2\n",
    "\n",
    "\n",
    "def get_combined_path(channel_path):\n",
    "    \"\"\"\n",
    "    Given a path to a channel, generates an *_all.pklz version of that path pointing\n",
    "    to the combined channels file.\n",
    "    \"\"\"\n",
    "    return re.sub(r'_[^._]*?\\.npz', '_all.pklz', channel_path)\n",
    "\n",
    "\n",
    "def connect_gcp(gcp_project, gcp_bucket_name):\n",
    "    \"\"\"\n",
    "    Connect to the Google Cloud Provider storage bucket.\n",
    "    \"\"\"\n",
    "    client = storage.Client(gcp_project)\n",
    "    bucket = client.get_bucket(gcp_bucket_name)\n",
    "    _logger.info('Connected to GCP storage bucket {}'.format(gcp_bucket_name))\n",
    "    return client, bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shuffling is being applied, this will alter the time sequence.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SDO_Dataset(gcp_project='space-weather-sdo',\n",
    "                            gcp_bucket_name='fdl-sdo-data',\n",
    "                            inventory_path='SDOMLnpz/inventory.pkl',\n",
    "                            batch_size=64,\n",
    "                            instr=['AIA'] * 7,\n",
    "                            channels=[\"0094\",\n",
    "                                      \"0131\",\n",
    "                                      \"0171\",\n",
    "                                      \"0193\",\n",
    "                                      \"0211\",\n",
    "                                      \"0304\",\n",
    "                                      \"0335\"],\n",
    "                            yr_range=[2012, 2013],\n",
    "                            mnt_step=1,\n",
    "                            day_step=1,\n",
    "                            h_step=6,\n",
    "                            min_step=60,\n",
    "                            resolution=512,\n",
    "                            subsample=4,\n",
    "                            normalization=0,\n",
    "                            scaling=True,\n",
    "                            test=False,\n",
    "                            test_ratio=0.3,\n",
    "                            shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dataloader_workers = multiprocessing.cpu_count() - 1\n",
    "train_loader = create_dataloader(train_dataset, num_dataloader_workers,\n",
    "                                 train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No blob for SDOMLnpz/2013/07/31/AIA20130731_0000_all.pklz\n",
      "No blob for SDOMLnpz/2012/06/14/AIA20120614_1200_all.pklz\n",
      "No blob for SDOMLnpz/2012/02/22/AIA20120222_0000_all.pklz\n",
      "No blob for SDOMLnpz/2013/04/23/AIA20130423_0000_all.pklz\n",
      "No blob for SDOMLnpz/2012/06/03/AIA20120603_0000_all.pklz\n",
      "No blob for SDOMLnpz/2012/05/27/AIA20120527_1200_all.pklz\n",
      "No blob for SDOMLnpz/2013/05/09/AIA20130509_0000_all.pklz\n",
      "No blob for SDOMLnpz/2012/03/22/AIA20120322_0000_all.pklz\n",
      "No blob for SDOMLnpz/2012/03/11/AIA20120311_0000_all.pklz\n",
      "No blob for SDOMLnpz/2012/07/22/AIA20120722_1200_all.pklz\n",
      "No blob for SDOMLnpz/2012/07/07/AIA20120707_1200_all.pklz\n",
      "No blob for SDOMLnpz/2012/04/27/AIA20120427_0000_all.pklz\n",
      "No blob for SDOMLnpz/2013/05/04/AIA20130504_1200_all.pklz\n",
      "No blob for SDOMLnpz/2013/06/12/AIA20130612_0000_all.pklz\n",
      "No blob for SDOMLnpz/2012/06/07/AIA20120607_1200_all.pklz\n",
      "No blob for SDOMLnpz/2012/04/12/AIA20120412_1200_all.pklz\n",
      "No blob for SDOMLnpz/2012/03/13/AIA20120313_1200_all.pklz\n",
      "No blob for SDOMLnpz/2013/07/08/AIA20130708_1200_all.pklz\n",
      "No blob for SDOMLnpz/2012/01/07/AIA20120107_0000_all.pklz\n",
      "No blob for SDOMLnpz/2013/03/02/AIA20130302_1200_all.pklz\n",
      "No blob for SDOMLnpz/2013/05/26/AIA20130526_1200_all.pklz\n",
      "No blob for SDOMLnpz/2013/07/01/AIA20130701_1200_all.pklz\n",
      "No blob for SDOMLnpz/2012/07/13/AIA20120713_0000_all.pklz\n",
      "No blob for SDOMLnpz/2012/03/03/AIA20120303_0000_all.pklz\n",
      "No blob for SDOMLnpz/2013/03/20/AIA20130320_0000_all.pklz\n",
      "No blob for SDOMLnpz/2013/06/11/AIA20130611_1200_all.pklz\n",
      "No blob for SDOMLnpz/2013/07/10/AIA20130710_1200_all.pklz\n",
      "No blob for SDOMLnpz/2012/06/02/AIA20120602_1200_all.pklz\n",
      "No blob for SDOMLnpz/2013/02/13/AIA20130213_0000_all.pklz\n",
      "No blob for SDOMLnpz/2013/03/30/AIA20130330_0000_all.pklz\n",
      "No blob for SDOMLnpz/2013/07/24/AIA20130724_0000_all.pklz\n",
      "No blob for SDOMLnpz/2013/07/16/AIA20130716_0000_all.pklz\n"
     ]
    }
   ],
   "source": [
    "log_interval = 10\n",
    "for batch_idx, (input_data,\n",
    "                gt_output,\n",
    "                optional_debug_data) in enumerate(train_loader):\n",
    "    print('batch_idx: {}'.format(batch_idx))\n",
    "\n",
    "    if batch_idx % log_interval == 0:\n",
    "        print('batch_idx: {}, input_data: {}, gt_output: {}'.format(batch_idx, input_data, gt_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
