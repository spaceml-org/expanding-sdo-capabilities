{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we want to try an experiment with time prediction of the Sun. A way to pass data by sequence is defined and some changes in the architecture but still some issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-07-24 02:35:38] DEBUG:matplotlib.pyplot:Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "from matplotlib.pyplot import imshow\n",
    "import operator\n",
    "from functools import reduce\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from sdo.sdo_dataset import SDO_Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from sdo.models.encoder_decoder import AutoEncoder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just a way to get nice logging messages from the sdo package\n",
    "logformat = \"[%(asctime)s] %(levelname)s:%(name)s:%(message)s\"\n",
    "logging.basicConfig(level=logging.DEBUG, stream=sys.stdout, format=logformat, datefmt=\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start simple by considering one single channel\n",
    "# single channel has a bug in the data loader (one xextra dim is added)\n",
    "subsample = 1\n",
    "original_ratio = 512\n",
    "img_shape = int(original_ratio/subsample)\n",
    "instr = ['AIA', 'AIA']\n",
    "channels = ['0171', '0131']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:5 for training, current device: 0, total devices: 6\n"
     ]
    }
   ],
   "source": [
    "#some cuda initialization\n",
    "torch.backends.cudnn.enabled = True\n",
    "cuda_device = 5\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"CUDA not available! Unable to continue\")\n",
    "device = torch.device(\"cuda:{}\".format(cuda_device))\n",
    "print(\"Using device {} for training, current device: {}, total devices: {}\".format(\n",
    "device, torch.cuda.current_device(), torch.cuda.device_count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-07-24 02:39:43] INFO:sdo.sdo_dataset:Loading SDOML from \"/gpfs/gpfs_gl4_16mb/b9p111/fdl_sw/SDOML\"\n",
      "[2019-07-24 02:39:43] INFO:sdo.sdo_dataset:Training on months \"[1 2 3 4 5 6 7]\"\n",
      "[2019-07-24 02:39:43] DEBUG:sdo.sdo_dataset:Timestamps requested values: \n",
      "[2019-07-24 02:39:43] DEBUG:sdo.sdo_dataset:Years: 2018\n",
      "[2019-07-24 02:39:43] DEBUG:sdo.sdo_dataset:Months: 1,2,3,4,5,6,7\n",
      "[2019-07-24 02:39:43] DEBUG:sdo.sdo_dataset:Days: 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31\n",
      "[2019-07-24 02:39:43] DEBUG:sdo.sdo_dataset:Hours: 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23\n",
      "[2019-07-24 02:39:43] DEBUG:sdo.sdo_dataset:Minutes: 0,12,24,36,48\n",
      "[2019-07-24 02:39:43] INFO:sdo.sdo_dataset:Max number of timestamps: 26040\n",
      "[2019-07-24 02:39:45] INFO:sdo.sdo_dataset:Timestamps found in the inventory: 24503 (0.94)\n",
      "[2019-07-24 02:39:49] INFO:sdo.sdo_dataset:N timestamps discarded because channel is missing = 20 (0.00082)\n",
      "[2019-07-24 02:39:49] INFO:sdo.sdo_dataset:Selected timestamps = 24483\n",
      "[2019-07-24 02:39:49] INFO:sdo.sdo_dataset:N images = 48966\n",
      "CPU times: user 6.87 s, sys: 322 ms, total: 7.19 s\n",
      "Wall time: 7.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_data = SDO_Dataset(device=device, instr=instr, channels=channels, yr_range=[2018,2018], \n",
    "                         mnt_step=1, day_step=1, h_step=1, min_step=12, subsample=subsample, \n",
    "                         test_ratio=0.3, normalization=0, scaling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this module we want to define an encoder/decoder RNN architecture\n",
    "\"\"\"\n",
    "def prod(iterable):\n",
    "    return reduce(operator.mul, iterable, 1)\n",
    "\n",
    "\n",
    "class EncoderDecoderRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    This is a RNN that output the next image of a sequence of images.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape=[1, 512, 512], lstm_input_dim=1024):\n",
    "        super(EncoderDecoderRNN, self).__init__()\n",
    "        self.lstm_input_dim = lstm_input_dim\n",
    "        self.num_channels = input_shape[0]\n",
    "        self.input_shape = input_shape\n",
    "        self.conv1 = nn.Conv2d(in_channels=self.num_channels, out_channels=32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3)\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3)\n",
    "        \n",
    "        self.dconv5 = nn.ConvTranspose2d(in_channels=64, out_channels=128, kernel_size=3)\n",
    "        self.dconv4 = nn.ConvTranspose2d(in_channels=128, out_channels=128, kernel_size=3)\n",
    "        self.dconv3 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3)\n",
    "        self.dconv2 = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=3)\n",
    "        self.dconv1 = nn.ConvTranspose2d(in_channels=32, out_channels=self.num_channels, kernel_size=3)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2, return_indices=True)\n",
    "        self.unpool = nn.MaxUnpool2d(2, 2)\n",
    "      \n",
    "        sample_encoder_input = torch.zeros(input_shape)\n",
    "        sample_encoder_output = self._encoder(sample_encoder_input.unsqueeze(0))[0]\n",
    "        sample_encoder_output_dim = sample_encoder_output.nelement()\n",
    "        print(\"sample_encoder_output_dim\",sample_encoder_output_dim)\n",
    "\n",
    "        # TODO: Choose a better hidden_size.\n",
    "        # TODO: Choose more stacked num_layers.\n",
    "        self.lin1 = nn.Linear(sample_encoder_output_dim,  self.lstm_input_dim)\n",
    "        self.lstm = nn.LSTM(input_size=self.lstm_input_dim, hidden_size=self.lstm_input_dim, \n",
    "                            num_layers=1, batch_first=True)\n",
    "        self.lstm_fc = nn.Linear(in_features=self.lstm_input_dim, out_features=sample_encoder_output_dim)\n",
    "\n",
    "        print('Autoencoder architecture:')\n",
    "        print('Input shape: {}'.format(input_shape))\n",
    "        print('Input dim  : {}'.format(prod(input_shape)))\n",
    "        print('Encoded dim: {}'.format(sample_encoder_output_dim))\n",
    "        print('Learnable params: {}'.format(sum([p.numel() for p in self.parameters()])))\n",
    "\n",
    "    def _encoder(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x, indices1 = self.pool(x)\n",
    "        x = self.conv3(x)\n",
    "        x, indices2 = self.pool(x)\n",
    "        x = self.conv4(x)\n",
    "        x, indices3 = self.pool(x)\n",
    "        x = self.conv5(x)\n",
    "        x, indices4 = self.pool(x)\n",
    "        x = F.relu(x)\n",
    "        return x, indices1, indices2, indices3, indices4\n",
    "\n",
    "    def _decoder(self, x, indices1, indices2, indices3, indices4):\n",
    "        # somthing is wrong here\n",
    "        x = x.view(64, 30, 30)\n",
    "        x = self.unpool(x, indices4)\n",
    "        x = self.dconv5(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.unpool(x, indices3)\n",
    "        x = self.dconv4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.unpool(x, indices2)\n",
    "        x = self.dconv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.unpool(x, indices1)\n",
    "        x = self.dconv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dconv1(x)\n",
    "        x = torch.relu(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x has shape B x T x C x H x W\n",
    "        #import pdb\n",
    "        #pdb.set_trace()\n",
    "        batch_size = x.shape[0]\n",
    "        sequence_size = x.shape[1]\n",
    "        encoded_x = []\n",
    "        decoded_x = []\n",
    "        indices_t = []\n",
    "        for t in range(sequence_size):\n",
    "            xt = x[:, t, :, :, :]\n",
    "            xt, indices1, indices2, indices3, indices4 = self._encoder(xt)\n",
    "            print(\"Encoded time stamp\", xt.shape)\n",
    "            Shap = xt.shape\n",
    "            xt = xt.view(Shap[0], -1)\n",
    "            xt = self.lin1(xt)\n",
    "            xt = F.relu(xt)\n",
    "            print(\"Encoded time stamp post FC\", xt.shape)\n",
    "            encoded_x.append(xt)\n",
    "            indices_t.append([indices1, indices2, indices3, indices4])\n",
    "            #xt = xt.view(Shap[0], 1, -1)\n",
    "        x = torch.stack(encoded_x)\n",
    "        x, _ = self.lstm(x)\n",
    "        print(\"Output LSTM\", x.shape)\n",
    "        x = self.lstm_fc(x)\n",
    "        print(\"Input decoder\", x.shape)\n",
    "        for t in range(sequence_size):\n",
    "            xt = x[t, :, :]\n",
    "            xt = self._decoder(xt, *indices_t[t])\n",
    "            decoded_x.append(xt)\n",
    "        x = torch.stack(decoded_x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_encoder_output_dim 57600\n",
      "Autoencoder architecture:\n",
      "Input shape: [2, 512, 512]\n",
      "Input dim  : 524288\n",
      "Encoded dim: 57600\n",
      "Learnable params: 127048834\n"
     ]
    }
   ],
   "source": [
    "#let's define a RNN model\n",
    "model = EncoderDecoderRNN(input_shape=[2, 512, 512])\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is the shuffling in the data loader shuffling the order of the batches or inside the batch\n",
    "seq_len = 6\n",
    "train_data_loader = DataLoader(train_data, batch_size=seq_len, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch torch.Size([1, 5, 2, 512, 512])\n",
      "Truth size torch.Size([1, 2, 512, 512])\n",
      "Batch train torch.Size([1, 5, 2, 512, 512])\n",
      "Encoded time stamp torch.Size([1, 64, 30, 30])\n",
      "Encoded time stamp post FC torch.Size([1, 1024])\n",
      "Encoded time stamp torch.Size([1, 64, 30, 30])\n",
      "Encoded time stamp post FC torch.Size([1, 1024])\n",
      "Encoded time stamp torch.Size([1, 64, 30, 30])\n",
      "Encoded time stamp post FC torch.Size([1, 1024])\n",
      "Encoded time stamp torch.Size([1, 64, 30, 30])\n",
      "Encoded time stamp post FC torch.Size([1, 1024])\n",
      "Encoded time stamp torch.Size([1, 64, 30, 30])\n",
      "Encoded time stamp post FC torch.Size([1, 1024])\n",
      "Output LSTM torch.Size([5, 1, 1024])\n",
      "Input decoder torch.Size([5, 1, 57600])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-f172c1dc6dd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtruth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtruth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fdl_sw/conda/envs/wmlce_py3_sdo/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-96-49d5eb88606d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mindices_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0mdecoded_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-96-49d5eb88606d>\u001b[0m in \u001b[0;36m_decoder\u001b[0;34m(self, x, indices1, indices2, indices3, indices4)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdconv5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fdl_sw/conda/envs/wmlce_py3_sdo/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fdl_sw/conda/envs/wmlce_py3_sdo/lib/python3.6/site-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, indices, output_size)\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         return F.max_unpool2d(input, indices, self.kernel_size, self.stride,\n\u001b[0;32m--> 366\u001b[0;31m                               self.padding, output_size)\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fdl_sw/conda/envs/wmlce_py3_sdo/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmax_unpool2d\u001b[0;34m(input, indices, kernel_size, stride, padding, output_size)\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m     output_size = _unpool_output_size(input, kernel_size, _stride, padding,\n\u001b[0;32m--> 609\u001b[0;31m                                       output_size)\n\u001b[0m\u001b[1;32m    610\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_unpool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fdl_sw/conda/envs/wmlce_py3_sdo/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_unpool_output_size\u001b[0;34m(input, kernel_size, stride, padding, output_size)\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         default_size.append((input_size[d + 2] - 1) * stride[d] +\n\u001b[0;32m--> 547\u001b[0;31m                             kernel_size[d] - 2 * padding[d])\n\u001b[0m\u001b[1;32m    548\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moutput_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "import pdb\n",
    "model.cuda(cuda_device)\n",
    "len_data = train_data.__len__()\n",
    "log_interval = 1\n",
    "# for now I am training on a single year\n",
    "n_epochs = 4\n",
    "train_loss = []\n",
    "for epoch in range(n_epochs):\n",
    "    for batch_index, batch in enumerate(train_data_loader):\n",
    "        # reshaping in order to get sequence\n",
    "        # batch_size define the length of the sequence\n",
    "        print(\"Batch\", batch_train.shape)\n",
    "        batch = batch.to(cuda_device)\n",
    "        batch = batch.unsqueeze(0)\n",
    "        truth = batch[:,-1,:, :, :]\n",
    "        print(\"Truth size\", truth.shape)\n",
    "        batch_train = batch[:, :-1, :, :, :]\n",
    "        print(\"Batch train\", batch_train.shape)\n",
    "        truth = truth.unsqueeze(1) \n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_train)\n",
    "        loss = distance(output, truth)\n",
    "        train_loss.append(float(loss))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_index % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_index * len(data), len_data, \n",
    "                100.*(batch_index* len(data)) / len_data, \n",
    "                loss.item() / len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
